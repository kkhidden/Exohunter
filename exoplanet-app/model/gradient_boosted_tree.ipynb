{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c344359f",
   "metadata": {},
   "source": [
    "# Enhanced Gradient Boosted Trees for NASA Exoplanet Classification\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates how to use **Gradient Boosted Trees** to classify exoplanets using NASA's Kepler mission data. We'll explore two powerful gradient boosting algorithms:\n",
    "\n",
    "- **XGBoost** (Extreme Gradient Boosting)\n",
    "- **LightGBM** (Light Gradient Boosting Machine)\n",
    "\n",
    "### What are Gradient Boosted Trees?\n",
    "\n",
    "Gradient Boosted Trees are a machine learning technique that combines multiple weak learners (simple decision trees) to create a strong predictive model. The key idea is to:\n",
    "\n",
    "1. Start with a simple model\n",
    "2. Identify where the model makes mistakes\n",
    "3. Train a new model to correct those mistakes\n",
    "4. Repeat this process many times\n",
    "5. Combine all models to make final predictions\n",
    "\n",
    "### Why Use Gradient Boosting for Exoplanet Classification?\n",
    "\n",
    "- **High Accuracy**: Often achieves state-of-the-art results on tabular data\n",
    "- **Handles Imbalanced Data**: Good at dealing with rare exoplanet signals\n",
    "- **Feature Importance**: Tells us which measurements are most important\n",
    "- **Robust to Noise**: Can handle measurement uncertainties in astronomical data\n",
    "\n",
    "### Our Goal\n",
    "\n",
    "We aim to achieve **80%+ F1 score** in classifying exoplanets using advanced techniques:\n",
    "- Hyperparameter optimization\n",
    "- Feature engineering\n",
    "- Model ensembles\n",
    "- Cross-validation strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6f4389",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup\n",
    "\n",
    "First, let's import all the necessary libraries. Don't worry if you're not familiar with all of them - we'll explain each one as we use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e683c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Basic libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Core data manipulation and numerical computing\n",
    "import pandas as pd  # For handling tabular data (like spreadsheets)\n",
    "import numpy as np   # For numerical computations and arrays\n",
    "\n",
    "# Machine learning tools from scikit-learn\n",
    "from sklearn.model_selection import GroupKFold, cross_val_score  # For model validation\n",
    "from sklearn.preprocessing import StandardScaler                 # For scaling features\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix  # For evaluating models\n",
    "from sklearn.utils.class_weight import compute_class_weight     # For handling imbalanced data\n",
    "\n",
    "# Hyperparameter optimization\n",
    "import optuna  # Automated hyperparameter tuning\n",
    "from optuna.samplers import TPESampler  # Tree-structured Parzen Estimator sampler\n",
    "\n",
    "# Utility libraries\n",
    "import warnings  # To suppress unnecessary warnings\n",
    "import time      # For timing our experiments\n",
    "import pickle    # For saving trained models\n",
    "import sys       # For system operations\n",
    "from pathlib import Path  # For file path operations\n",
    "\n",
    "# Add our dataset loader to the Python path\n",
    "sys.path.append('/Users/kkgogada/Code/NASASAC2025')\n",
    "from dataset.loader import KeplerDatasetLoader\n",
    "\n",
    "print(\"Basic libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acce33e",
   "metadata": {},
   "source": [
    "### Loading Gradient Boosting Libraries\n",
    "\n",
    "Now let's try to import the gradient boosting libraries. These are specialized libraries that might need to be installed separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "663d79cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost available - This is a popular gradient boosting library developed by the University of Washington\n",
      "LightGBM available - This is Microsoft's fast gradient boosting library\n",
      "\n",
      "Ready to build some gradient boosted models!\n"
     ]
    }
   ],
   "source": [
    "# Try to import XGBoost\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGBOOST_AVAILABLE = True\n",
    "    print(\"XGBoost available - This is a popular gradient boosting library developed by the University of Washington\")\n",
    "except ImportError as e:\n",
    "    print(f\"XGBoost not available: {e}\")\n",
    "    print(\"   You can install it with: pip install xgboost\")\n",
    "    XGBOOST_AVAILABLE = False\n",
    "\n",
    "# Try to import LightGBM\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    LIGHTGBM_AVAILABLE = True\n",
    "    print(\"LightGBM available - This is Microsoft's fast gradient boosting library\")\n",
    "except ImportError as e:\n",
    "    print(f\"LightGBM not available: {e}\")\n",
    "    print(\"   You can install it with: pip install lightgbm\")\n",
    "    LIGHTGBM_AVAILABLE = False\n",
    "\n",
    "# Check if we have at least one gradient boosting library\n",
    "if not (XGBOOST_AVAILABLE or LIGHTGBM_AVAILABLE):\n",
    "    print(\"\\nNo gradient boosting libraries available!\")\n",
    "    print(\"Please install at least one of: xgboost, lightgbm\")\n",
    "else:\n",
    "    print(\"\\nReady to build some gradient boosted models!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46abfa5",
   "metadata": {},
   "source": [
    "## 2. Understanding the Data\n",
    "\n",
    "Before we build our models, let's load and examine the exoplanet data to understand what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eccfb6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dataset.loader:Loading from cache: /Users/kkgogada/Code/NASASAC2025/dataset/kepler_processed.parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Loading NASA Kepler Exoplanet Dataset...\n",
      "\n",
      "üìà Dataset Overview:\n",
      "   ‚Ä¢ Total samples: 9,564\n",
      "   ‚Ä¢ Number of features: 61\n",
      "   ‚Ä¢ Number of star groups: 8214\n",
      "\n",
      "üéØ Class Distribution:\n",
      "   ‚Ä¢ Class 0 (False Positive): 1,979 samples (20.7%)\n",
      "   ‚Ä¢ Class 1 (Confirmed Planet): 2,746 samples (28.7%)\n",
      "   ‚Ä¢ Class 2 (Planet Candidate): 4,839 samples (50.6%)\n",
      "\n",
      "üìà Dataset Overview:\n",
      "   ‚Ä¢ Total samples: 9,564\n",
      "   ‚Ä¢ Number of features: 61\n",
      "   ‚Ä¢ Number of star groups: 8214\n",
      "\n",
      "üéØ Class Distribution:\n",
      "   ‚Ä¢ Class 0 (False Positive): 1,979 samples (20.7%)\n",
      "   ‚Ä¢ Class 1 (Confirmed Planet): 2,746 samples (28.7%)\n",
      "   ‚Ä¢ Class 2 (Planet Candidate): 4,839 samples (50.6%)\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "print(\"Loading NASA Kepler Exoplanet Dataset...\")\n",
    "loader = KeplerDatasetLoader()\n",
    "X, y, groups, feature_names = loader.load_dataset()\n",
    "\n",
    "print(f\"\\nDataset Overview:\")\n",
    "print(f\"   ‚Ä¢ Total samples: {X.shape[0]:,}\")\n",
    "print(f\"   ‚Ä¢ Number of features: {X.shape[1]}\")\n",
    "print(f\"   ‚Ä¢ Number of star groups: {len(np.unique(groups))}\")\n",
    "\n",
    "# Convert labels to integers to ensure compatibility\n",
    "y = y.astype(int) if hasattr(y, 'astype') else pd.Series(y).astype(int)\n",
    "\n",
    "# Show class distribution\n",
    "class_counts = pd.Series(y).value_counts().sort_index()\n",
    "print(f\"\\nClass Distribution:\")\n",
    "class_names = {0: 'False Positive', 1: 'Confirmed Planet', 2: 'Planet Candidate'}\n",
    "for class_id, count in class_counts.items():\n",
    "    percentage = (count / len(y)) * 100\n",
    "    print(f\"   ‚Ä¢ Class {class_id} ({class_names.get(int(class_id), 'Unknown')}): {count:,} samples ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d7f365",
   "metadata": {},
   "source": [
    "### Exploring the Features\n",
    "\n",
    "Let's examine what measurements (features) we have for each potential exoplanet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1649d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Available Features (Measurements):\n",
      "\n",
      "These are the astronomical measurements we'll use to identify exoplanets:\n",
      "\n",
      "              period     duration         depth  impact_param  planet_radius  \\\n",
      "count    9564.000000  9564.000000  9.564000e+03   9564.000000    9564.000000   \n",
      "mean       75.671358     5.621606  2.290432e+04      0.727586      99.077250   \n",
      "std      1334.744046     6.471554  8.079020e+04      3.284876    3018.723391   \n",
      "min         0.241843     0.052000  0.000000e+00      0.000000       0.080000   \n",
      "25%         2.733684     2.437750  1.668000e+02      0.209075       1.430000   \n",
      "50%         9.752831     3.792600  4.211000e+02      0.537000       2.390000   \n",
      "75%        40.715178     6.276500  1.341775e+03      0.877000      13.112500   \n",
      "max    129995.778400   138.540000  1.541400e+06    100.806000  200346.000000   \n",
      "\n",
      "       semi_major_axis  equilibrium_temp    insolation          snr  \\\n",
      "count      9564.000000       9564.000000  9.564000e+03  9564.000000   \n",
      "mean          0.218717       1077.514534  7.490517e+03   250.903691   \n",
      "std           0.556140        840.875323  1.565158e+05   781.868222   \n",
      "min           0.005900         25.000000  0.000000e+00     0.000000   \n",
      "25%           0.039300        553.000000  2.216000e+01    12.300000   \n",
      "50%           0.085100        878.000000  1.416000e+02    23.000000   \n",
      "75%           0.203450       1352.500000  8.067975e+02    71.125000   \n",
      "max          44.989200      14667.000000  1.094755e+07  9054.700000   \n",
      "\n",
      "       num_transits  ...  quarters_missing  radius_ratio_missing  \\\n",
      "count   9564.000000  ...       9564.000000           9564.000000   \n",
      "mean     356.109682  ...          0.119406              0.037955   \n",
      "std      518.111875  ...          0.324283              0.191097   \n",
      "min        0.000000  ...          0.000000              0.000000   \n",
      "25%       52.000000  ...          0.000000              0.000000   \n",
      "50%      143.000000  ...          0.000000              0.000000   \n",
      "75%      382.250000  ...          0.000000              0.000000   \n",
      "max     2664.000000  ...          1.000000              1.000000   \n",
      "\n",
      "       semi_major_axis_ratio_missing  period_log10  depth_log10  \\\n",
      "count                    9564.000000   9564.000000  9564.000000   \n",
      "mean                        0.037955      1.067132     2.875342   \n",
      "std                         0.191097      0.819351     1.007685   \n",
      "min                         0.000000     -0.616467    -0.096910   \n",
      "25%                         0.000000      0.436748     2.222391   \n",
      "50%                         0.000000      0.989131     2.624385   \n",
      "75%                         0.000000      1.609756     3.127679   \n",
      "max                         1.000000      5.113929     6.187915   \n",
      "\n",
      "       planet_radius_log10  semi_major_axis_log10  insolation_log10  \\\n",
      "count          9564.000000            9564.000000       9564.000000   \n",
      "mean              0.637444              -1.014257          2.108477   \n",
      "std               0.694294               0.534782          1.186685   \n",
      "min              -1.096910              -2.229148         -1.698970   \n",
      "25%               0.155336              -1.405607          1.345913   \n",
      "50%               0.378398              -1.070070          2.151063   \n",
      "75%               1.117685              -0.691542          2.906765   \n",
      "max               5.301781               1.653108          7.039317   \n",
      "\n",
      "       equilibrium_temp_log10  stellar_mass_log10  \n",
      "count             9564.000000         9564.000000  \n",
      "mean                 2.932376           -0.010515  \n",
      "std                  0.296086            0.130571  \n",
      "min                  1.397940           -1.026872  \n",
      "25%                  2.742725           -0.069051  \n",
      "50%                  2.943495           -0.011441  \n",
      "75%                  3.131137            0.039414  \n",
      "max                  4.166341            0.572291  \n",
      "\n",
      "[8 rows x 61 columns]\n",
      "\n",
      "üìù Feature Explanations:\n",
      "   ‚Ä¢ period: Orbital period - how long it takes the planet to orbit its star (days)\n",
      "   ‚Ä¢ duration: Transit duration - how long the dimming lasts (hours)\n",
      "   ‚Ä¢ depth: Transit depth - how much the star dims when the planet passes in front (%)\n",
      "   ‚Ä¢ planet_radius: Estimated radius of the planet (Earth radii)\n",
      "   ‚Ä¢ snr: Signal-to-noise ratio - how clear the signal is above background noise\n",
      "   ‚Ä¢ stellar_radius: Radius of the host star (Solar radii)\n",
      "   ‚Ä¢ stellar_mass: Mass of the host star (Solar masses)\n"
     ]
    }
   ],
   "source": [
    "# Display feature information\n",
    "print(\"Available Features (Measurements):\")\n",
    "print(\"\\nThese are the astronomical measurements we'll use to identify exoplanets:\\n\")\n",
    "\n",
    "# Create a DataFrame to better display the data\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "\n",
    "# Show basic statistics\n",
    "print(df.describe())\n",
    "\n",
    "print(\"\\nFeature Explanations:\")\n",
    "feature_explanations = {\n",
    "    'period': 'Orbital period - how long it takes the planet to orbit its star (days)',\n",
    "    'depth': 'Transit depth - how much the star dims when the planet passes in front (%)',\n",
    "    'duration': 'Transit duration - how long the dimming lasts (hours)',\n",
    "    'snr': 'Signal-to-noise ratio - how clear the signal is above background noise',\n",
    "    'planet_radius': 'Estimated radius of the planet (Earth radii)',\n",
    "    'stellar_radius': 'Radius of the host star (Solar radii)',\n",
    "    'stellar_mass': 'Mass of the host star (Solar masses)',\n",
    "    'stellar_temp': 'Temperature of the host star (Kelvin)'\n",
    "}\n",
    "\n",
    "for feature in feature_names:\n",
    "    if feature in feature_explanations:\n",
    "        print(f\"   - {feature}: {feature_explanations[feature]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48becfd5",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "\n",
    "**Feature Engineering** is the process of creating new, more informative features from existing data. This is crucial for gradient boosted trees because:\n",
    "\n",
    "1. **Better Patterns**: New features can reveal hidden patterns\n",
    "2. **Domain Knowledge**: We can incorporate astronomical knowledge\n",
    "3. **Improved Performance**: Better features = better predictions\n",
    "\n",
    "Let's create some new features that might help identify exoplanets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591cce56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Applying Feature Engineering...\n",
      "\n",
      "1Ô∏è‚É£ Creating Ratio Features:\n",
      "   These ratios often reveal important relationships in astronomy\n",
      "   ‚Ä¢ Period/Duration ratio - indicates how long the transit is relative to orbital period\n",
      "   ‚Ä¢ Depth/SNR ratio - helps distinguish real signals from noise\n",
      "   ‚Ä¢ Planet/Star radius ratio - fundamental for transit depth calculations\n",
      "\n",
      "2Ô∏è‚É£ Creating Logarithmic Features:\n",
      "   Many astronomical quantities follow log-normal distributions\n",
      "   ‚Ä¢ Log of period - reduces the effect of extreme values\n",
      "   ‚Ä¢ Log of depth - reduces the effect of extreme values\n",
      "   ‚Ä¢ Log of duration - reduces the effect of extreme values\n",
      "   ‚Ä¢ Log of snr - reduces the effect of extreme values\n",
      "\n",
      "3Ô∏è‚É£ Creating Binned Features:\n",
      "   Binning can help tree models by creating cleaner decision boundaries\n",
      "   ‚Ä¢ Period bins - groups similar orbital periods together\n",
      "   ‚Ä¢ Depth bins - groups similar transit depths together\n",
      "\n",
      "‚úÖ Feature engineering complete!\n",
      "   Original features: 61\n",
      "   Enhanced features: 69\n",
      "   New features added: 8\n"
     ]
    }
   ],
   "source": [
    "def feature_engineering(X):\n",
    "    \"\"\"\n",
    "    Create new features from existing measurements.\n",
    "    This function applies domain knowledge to generate more informative features.\n",
    "    \"\"\"\n",
    "    print(\"Applying Feature Engineering...\")\n",
    "    \n",
    "    # Start with a copy of the original data\n",
    "    X_enhanced = X.copy()\n",
    "    \n",
    "    print(\"\\nCreating Ratio Features:\")\n",
    "    print(\"   These ratios often reveal important relationships in astronomy\")\n",
    "    \n",
    "    # 1. Period to Duration Ratio\n",
    "    if 'period' in X.columns and 'duration' in X.columns:\n",
    "        X_enhanced['period_duration_ratio'] = X['period'] / (X['duration'] + 1e-8)\n",
    "        print(\"   ‚Ä¢ Period/Duration ratio - indicates how long the transit is relative to orbital period\")\n",
    "        \n",
    "    # 2. Depth to Signal-to-Noise Ratio\n",
    "    if 'depth' in X.columns and 'snr' in X.columns:\n",
    "        X_enhanced['depth_snr_ratio'] = X['depth'] / (X['snr'] + 1e-8)\n",
    "        print(\"   ‚Ä¢ Depth/SNR ratio - helps distinguish real signals from noise\")\n",
    "        \n",
    "    # 3. Planet to Star Radius Ratio\n",
    "    if 'planet_radius' in X.columns and 'stellar_radius' in X.columns:\n",
    "        X_enhanced['radius_ratio'] = X['planet_radius'] / (X['stellar_radius'] + 1e-8)\n",
    "        print(\"   ‚Ä¢ Planet/Star radius ratio - fundamental for transit depth calculations\")\n",
    "    \n",
    "    print(\"\\nCreating Logarithmic Features:\")\n",
    "    print(\"   Many astronomical quantities follow log-normal distributions\")\n",
    "    \n",
    "    # 2. Log transformations for skewed features\n",
    "    skewed_features = ['period', 'depth', 'duration', 'snr']\n",
    "    for feat in skewed_features:\n",
    "        if feat in X.columns:\n",
    "            X_enhanced[f'{feat}_log'] = np.log1p(X[feat])  # log1p = log(1 + x), handles zeros\n",
    "            print(f\"   ‚Ä¢ Log of {feat} - reduces the effect of extreme values\")\n",
    "    \n",
    "    print(\"\\n Creating Binned Features:\")\n",
    "    print(\"   Binning can help tree models by creating cleaner decision boundaries\")\n",
    "    \n",
    "    # 3. Binning for tree models\n",
    "    if 'period' in X.columns:\n",
    "        X_enhanced['period_bin'] = pd.cut(X['period'], bins=10, labels=False)\n",
    "        print(\"   ‚Ä¢ Period bins - groups similar orbital periods together\")\n",
    "        \n",
    "    if 'depth' in X.columns:\n",
    "        X_enhanced['depth_bin'] = pd.cut(X['depth'], bins=10, labels=False)\n",
    "        print(\"   ‚Ä¢ Depth bins - groups similar transit depths together\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Feature engineering complete!\")\n",
    "    print(f\"   Original features: {X.shape[1]}\")\n",
    "    print(f\"   Enhanced features: {X_enhanced.shape[1]}\")\n",
    "    print(f\"   New features added: {X_enhanced.shape[1] - X.shape[1]}\")\n",
    "    \n",
    "    return X_enhanced\n",
    "\n",
    "# Apply feature engineering\n",
    "X_enhanced = feature_engineering(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dded8ef5",
   "metadata": {},
   "source": [
    "## 4. Handling Class Imbalance\n",
    "\n",
    "In exoplanet detection, we have an **imbalanced dataset** - there are many more false positives than confirmed planets. This is realistic since:\n",
    "\n",
    "- Planets are rare\n",
    "- Many signals turn out to be false alarms\n",
    "- Confirmation requires extensive follow-up\n",
    "\n",
    "We'll use **class weights** to help our model pay more attention to the rare, important cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104f0551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öñÔ∏è Calculating Class Weights for Imbalanced Data...\n",
      "\n",
      "üìä Class Weights (higher = more important):\n",
      "   ‚Ä¢ Class 0 (False Positive): 1.611\n",
      "   ‚Ä¢ Class 1 (Confirmed Planet): 1.161\n",
      "   ‚Ä¢ Class 2 (Planet Candidate): 0.659\n",
      "\n",
      "üí° Why Class Weights Matter:\n",
      "   ‚Ä¢ Higher weights make the model pay more attention to rare classes\n",
      "   ‚Ä¢ This prevents the model from just predicting the most common class\n",
      "   ‚Ä¢ Essential for detecting rare but important exoplanets\n"
     ]
    }
   ],
   "source": [
    "# Calculate class weights to handle imbalanced data\n",
    "print(\"Calculating Class Weights for Imbalanced Data...\")\n",
    "\n",
    "classes = np.unique(y)\n",
    "class_weights = dict(zip(classes, compute_class_weight('balanced', classes=classes, y=y)))\n",
    "\n",
    "print(\"\\nüìä Class Weights (higher = more important):\")\n",
    "for class_id, weight in class_weights.items():\n",
    "    class_name = class_names.get(class_id, 'Unknown')\n",
    "    print(f\"   ‚Ä¢ Class {class_id} ({class_name}): {weight:.3f}\")\n",
    "\n",
    "print(\"\\nüí° Why Class Weights Matter:\")\n",
    "print(\"   ‚Ä¢ Higher weights make the model pay more attention to rare classes\")\n",
    "print(\"   ‚Ä¢ This prevents the model from just predicting the most common class\")\n",
    "print(\"   ‚Ä¢ Essential for detecting rare but important exoplanets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d92e16",
   "metadata": {},
   "source": [
    "## 5. Hyperparameter Optimization with Optuna\n",
    "\n",
    "**Hyperparameters** are the settings we choose for our machine learning models (like learning rate, tree depth, etc.). Finding the best hyperparameters is crucial for good performance.\n",
    "\n",
    "We'll use **Optuna**, an automatic hyperparameter optimization framework that intelligently searches for the best settings.\n",
    "\n",
    "### How Optuna Works:\n",
    "1. **Try different parameter combinations**\n",
    "2. **Evaluate each combination** using cross-validation\n",
    "3. **Learn from results** to suggest better combinations\n",
    "4. **Repeat** until we find the best parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401c7ed9",
   "metadata": {},
   "source": [
    "### Optimizing XGBoost Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5869e0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 08:22:47,692] A new study created in memory with name: no-name-c2ee5897-8de7-426d-b7ce-9ef4cae53275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Optimizing XGBoost Hyperparameters...\n",
      "   Will try 50 different parameter combinations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.773279:   2%|‚ñè         | 1/50 [00:25<20:53, 25.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 08:23:13,289] Trial 0 finished with value: 0.7732790827852535 and parameters: {'n_estimators': 437, 'max_depth': 10, 'learning_rate': 0.1205712628744377, 'subsample': 0.8394633936788146, 'colsample_bytree': 0.6624074561769746, 'reg_alpha': 2.5348407664333426e-07, 'reg_lambda': 3.3323645788192616e-08, 'min_child_weight': 9}. Best is trial 0 with value: 0.7732790827852535.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.773279:   4%|‚ñç         | 2/50 [01:37<42:27, 53.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 08:24:25,609] Trial 1 finished with value: 0.771990826215698 and parameters: {'n_estimators': 641, 'max_depth': 8, 'learning_rate': 0.010725209743171997, 'subsample': 0.9879639408647978, 'colsample_bytree': 0.9329770563201687, 'reg_alpha': 8.148018307012941e-07, 'reg_lambda': 4.329370014459266e-07, 'min_child_weight': 2}. Best is trial 0 with value: 0.7732790827852535.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.773279:   6%|‚ñå         | 3/50 [02:01<31:04, 39.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 08:24:49,315] Trial 2 finished with value: 0.7729085604365791 and parameters: {'n_estimators': 374, 'max_depth': 7, 'learning_rate': 0.04345454109729477, 'subsample': 0.7164916560792167, 'colsample_bytree': 0.8447411578889518, 'reg_alpha': 1.8007140198129195e-07, 'reg_lambda': 4.258943089524393e-06, 'min_child_weight': 4}. Best is trial 0 with value: 0.7732790827852535.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.77445:   8%|‚ñä         | 4/50 [03:21<42:41, 55.69s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 08:26:09,569] Trial 3 finished with value: 0.7744499579656834 and parameters: {'n_estimators': 510, 'max_depth': 9, 'learning_rate': 0.019721610970574007, 'subsample': 0.8056937753654446, 'colsample_bytree': 0.836965827544817, 'reg_alpha': 2.6185068507773707e-08, 'reg_lambda': 0.0029369981104377003, 'min_child_weight': 2}. Best is trial 3 with value: 0.7744499579656834.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.77445:  10%|‚ñà         | 5/50 [03:35<30:28, 40.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 08:26:23,507] Trial 4 finished with value: 0.769798676139709 and parameters: {'n_estimators': 158, 'max_depth': 10, 'learning_rate': 0.26690431824362526, 'subsample': 0.9233589392465844, 'colsample_bytree': 0.7218455076693483, 'reg_alpha': 7.569183361880229e-08, 'reg_lambda': 0.014391207615728067, 'min_child_weight': 5}. Best is trial 3 with value: 0.7744499579656834.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.77445:  12%|‚ñà‚ñè        | 6/50 [03:50<23:14, 31.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 08:26:37,849] Trial 5 finished with value: 0.7567835423500359 and parameters: {'n_estimators': 209, 'max_depth': 6, 'learning_rate': 0.011240768803005551, 'subsample': 0.9637281608315128, 'colsample_bytree': 0.7035119926400067, 'reg_alpha': 0.009176996354542699, 'reg_lambda': 6.388511557344611e-06, 'min_child_weight': 6}. Best is trial 3 with value: 0.7744499579656834.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.77445:  14%|‚ñà‚ñç        | 7/50 [04:11<20:16, 28.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 08:26:59,135] Trial 6 finished with value: 0.7626440941085262 and parameters: {'n_estimators': 592, 'max_depth': 4, 'learning_rate': 0.27051668818999286, 'subsample': 0.9100531293444458, 'colsample_bytree': 0.9757995766256756, 'reg_alpha': 1.1309571585271483, 'reg_lambda': 0.002404915432737351, 'min_child_weight': 10}. Best is trial 3 with value: 0.7744499579656834.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.77445:  16%|‚ñà‚ñå        | 8/50 [04:17<14:57, 21.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 08:27:05,675] Trial 7 finished with value: 0.7386862326261755 and parameters: {'n_estimators': 179, 'max_depth': 4, 'learning_rate': 0.011662890273931383, 'subsample': 0.7301321323053057, 'colsample_bytree': 0.7554709158757928, 'reg_alpha': 2.7678419414850017e-06, 'reg_lambda': 0.28749982347407854, 'min_child_weight': 4}. Best is trial 3 with value: 0.7744499579656834.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.77445:  18%|‚ñà‚ñä        | 9/50 [04:39<14:38, 21.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 08:27:27,217] Trial 8 finished with value: 0.7615339856734169 and parameters: {'n_estimators': 353, 'max_depth': 7, 'learning_rate': 0.016149614799999188, 'subsample': 0.9208787923016158, 'colsample_bytree': 0.6298202574719083, 'reg_alpha': 7.620481786158549, 'reg_lambda': 0.08916674715636537, 'min_child_weight': 2}. Best is trial 3 with value: 0.7744499579656834.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 9. Best value: 0.775844:  20%|‚ñà‚ñà        | 10/50 [04:51<12:25, 18.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 08:27:39,656] Trial 9 finished with value: 0.7758444836244629 and parameters: {'n_estimators': 104, 'max_depth': 9, 'learning_rate': 0.11069143219393454, 'subsample': 0.8916028672163949, 'colsample_bytree': 0.9085081386743783, 'reg_alpha': 4.638759594322625e-08, 'reg_lambda': 1.683416412018213e-05, 'min_child_weight': 2}. Best is trial 9 with value: 0.7758444836244629.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 9. Best value: 0.775844:  22%|‚ñà‚ñà‚ñè       | 11/50 [05:28<15:35, 23.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 08:28:15,712] Trial 10 finished with value: 0.7738434795622708 and parameters: {'n_estimators': 891, 'max_depth': 6, 'learning_rate': 0.08861501021155405, 'subsample': 0.6071847502459278, 'colsample_bytree': 0.9016552640704525, 'reg_alpha': 5.9361246269485385e-05, 'reg_lambda': 4.3444691085504115, 'min_child_weight': 7}. Best is trial 9 with value: 0.7758444836244629.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 9. Best value: 0.775844:  24%|‚ñà‚ñà‚ñç       | 12/50 [06:45<25:29, 40.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 08:29:33,205] Trial 11 finished with value: 0.775599108825224 and parameters: {'n_estimators': 806, 'max_depth': 9, 'learning_rate': 0.034593301147901656, 'subsample': 0.8234098923117161, 'colsample_bytree': 0.8338781008362445, 'reg_alpha': 2.146157911041442e-08, 'reg_lambda': 0.00013730750409712947, 'min_child_weight': 1}. Best is trial 9 with value: 0.7758444836244629.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 12. Best value: 0.776165:  26%|‚ñà‚ñà‚ñå       | 13/50 [08:07<32:41, 53.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 08:30:55,568] Trial 12 finished with value: 0.776164703128474 and parameters: {'n_estimators': 822, 'max_depth': 9, 'learning_rate': 0.0391334743391237, 'subsample': 0.856161023018213, 'colsample_bytree': 0.8743242710262783, 'reg_alpha': 3.3700898622196565e-05, 'reg_lambda': 5.533029685493348e-05, 'min_child_weight': 1}. Best is trial 12 with value: 0.776164703128474.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 12. Best value: 0.776165:  28%|‚ñà‚ñà‚ñä       | 14/50 [09:00<31:39, 52.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 08:31:47,728] Trial 13 finished with value: 0.7682801450210126 and parameters: {'n_estimators': 984, 'max_depth': 8, 'learning_rate': 0.10911097866235923, 'subsample': 0.8695456595762009, 'colsample_bytree': 0.8842401657774551, 'reg_alpha': 0.00012849617790740246, 'reg_lambda': 8.77347226309253e-05, 'min_child_weight': 1}. Best is trial 12 with value: 0.776164703128474.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 12. Best value: 0.776165:  30%|‚ñà‚ñà‚ñà       | 15/50 [09:50<30:25, 52.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 08:32:38,498] Trial 14 finished with value: 0.7709048127500646 and parameters: {'n_estimators': 709, 'max_depth': 9, 'learning_rate': 0.0656703027839698, 'subsample': 0.7287793721266961, 'colsample_bytree': 0.9927155179690936, 'reg_alpha': 0.0042586797512623705, 'reg_lambda': 2.9501614337602306e-06, 'min_child_weight': 3}. Best is trial 12 with value: 0.776164703128474.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 15. Best value: 0.777235:  32%|‚ñà‚ñà‚ñà‚ñè      | 16/50 [11:04<33:15, 58.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 08:33:52,361] Trial 15 finished with value: 0.777235047958514 and parameters: {'n_estimators': 780, 'max_depth': 8, 'learning_rate': 0.028865741076034596, 'subsample': 0.7687352087510899, 'colsample_bytree': 0.7843562856440282, 'reg_alpha': 2.914351403702614e-05, 'reg_lambda': 6.364501631001885e-05, 'min_child_weight': 1}. Best is trial 15 with value: 0.777235047958514.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 15. Best value: 0.777235:  34%|‚ñà‚ñà‚ñà‚ñç      | 17/50 [11:28<26:27, 48.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 08:34:15,816] Trial 16 finished with value: 0.7718865994258008 and parameters: {'n_estimators': 808, 'max_depth': 5, 'learning_rate': 0.02741025687637504, 'subsample': 0.7691423653035051, 'colsample_bytree': 0.7522463242930172, 'reg_alpha': 8.219002563274163e-06, 'reg_lambda': 1.1032173338053817e-08, 'min_child_weight': 7}. Best is trial 15 with value: 0.777235047958514.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 15. Best value: 0.777235:  36%|‚ñà‚ñà‚ñà‚ñå      | 18/50 [12:12<24:59, 46.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 08:34:59,815] Trial 17 finished with value: 0.776275494726957 and parameters: {'n_estimators': 749, 'max_depth': 8, 'learning_rate': 0.02454027805821759, 'subsample': 0.6732132011401761, 'colsample_bytree': 0.791311085108537, 'reg_alpha': 0.0020421344670930462, 'reg_lambda': 0.0007368111292097674, 'min_child_weight': 4}. Best is trial 15 with value: 0.777235047958514.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 15. Best value: 0.777235:  38%|‚ñà‚ñà‚ñà‚ñä      | 19/50 [12:28<19:25, 37.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 08:35:15,851] Trial 18 finished with value: 0.7605491081418635 and parameters: {'n_estimators': 984, 'max_depth': 3, 'learning_rate': 0.021949563984953718, 'subsample': 0.6324203146433717, 'colsample_bytree': 0.7779624618672082, 'reg_alpha': 0.0013026294604439066, 'reg_lambda': 0.0014561469589187465, 'min_child_weight': 4}. Best is trial 15 with value: 0.777235047958514.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 15. Best value: 0.777235:  40%|‚ñà‚ñà‚ñà‚ñà      | 20/50 [12:59<17:47, 35.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 08:35:46,774] Trial 19 finished with value: 0.7722656806482611 and parameters: {'n_estimators': 643, 'max_depth': 8, 'learning_rate': 0.061590213651042024, 'subsample': 0.6620897622895917, 'colsample_bytree': 0.808298067259796, 'reg_alpha': 0.05190925408128177, 'reg_lambda': 3.345470520860881e-07, 'min_child_weight': 6}. Best is trial 15 with value: 0.777235047958514.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 15. Best value: 0.777235:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 21/50 [13:42<18:16, 37.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 08:36:29,715] Trial 20 finished with value: 0.7753100585582641 and parameters: {'n_estimators': 713, 'max_depth': 7, 'learning_rate': 0.026531896536455553, 'subsample': 0.6805454483691531, 'colsample_bytree': 0.6873064153189612, 'reg_alpha': 0.10344069793225955, 'reg_lambda': 0.0005961727498011823, 'min_child_weight': 3}. Best is trial 15 with value: 0.777235047958514.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 15. Best value: 0.777235:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 22/50 [14:54<22:31, 48.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 08:37:42,442] Trial 21 finished with value: 0.7747172117485073 and parameters: {'n_estimators': 815, 'max_depth': 8, 'learning_rate': 0.04142795397056939, 'subsample': 0.7546516529555951, 'colsample_bytree': 0.794701362470173, 'reg_alpha': 2.5063987657281907e-05, 'reg_lambda': 4.514948377724551e-05, 'min_child_weight': 1}. Best is trial 15 with value: 0.777235047958514.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 15. Best value: 0.777235:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 23/50 [15:59<23:54, 53.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 08:38:46,885] Trial 22 finished with value: 0.7738549064250295 and parameters: {'n_estimators': 886, 'max_depth': 10, 'learning_rate': 0.033224624074375125, 'subsample': 0.7762511680322971, 'colsample_bytree': 0.8738445964974808, 'reg_alpha': 0.00044313008341208444, 'reg_lambda': 0.0002543262448784436, 'min_child_weight': 3}. Best is trial 15 with value: 0.777235047958514.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 15. Best value: 0.777235:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 24/50 [17:01<24:11, 55.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 08:39:49,013] Trial 23 finished with value: 0.7762320543384148 and parameters: {'n_estimators': 728, 'max_depth': 8, 'learning_rate': 0.049775628265661315, 'subsample': 0.6858782210323644, 'colsample_bytree': 0.9449923596722283, 'reg_alpha': 0.0007696437393140442, 'reg_lambda': 0.022668894014942893, 'min_child_weight': 1}. Best is trial 15 with value: 0.777235047958514.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 15. Best value: 0.777235:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 25/50 [17:41<21:18, 51.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 08:40:29,270] Trial 24 finished with value: 0.774381826853237 and parameters: {'n_estimators': 724, 'max_depth': 7, 'learning_rate': 0.015783128498238316, 'subsample': 0.6840336109427874, 'colsample_bytree': 0.9450588707370331, 'reg_alpha': 0.0012365915113348675, 'reg_lambda': 0.018768361986578216, 'min_child_weight': 5}. Best is trial 15 with value: 0.777235047958514.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 15. Best value: 0.777235:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 26/50 [18:14<18:13, 45.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 08:41:01,798] Trial 25 finished with value: 0.7746282407876832 and parameters: {'n_estimators': 537, 'max_depth': 8, 'learning_rate': 0.05342424753981705, 'subsample': 0.6432375260318544, 'colsample_bytree': 0.7628211189695865, 'reg_alpha': 0.022849116912737305, 'reg_lambda': 0.8361680353866072, 'min_child_weight': 3}. Best is trial 15 with value: 0.777235047958514.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 15. Best value: 0.777235:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 27/50 [18:42<15:30, 40.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 08:41:30,339] Trial 26 finished with value: 0.7689535270449016 and parameters: {'n_estimators': 753, 'max_depth': 6, 'learning_rate': 0.17112516552161885, 'subsample': 0.7047533705605785, 'colsample_bytree': 0.7362209572902984, 'reg_alpha': 0.0003827614025939607, 'reg_lambda': 0.02268540331922873, 'min_child_weight': 2}. Best is trial 15 with value: 0.777235047958514.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 27. Best value: 0.778684:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 28/50 [19:59<18:51, 51.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 08:42:47,416] Trial 27 finished with value: 0.7786842789109854 and parameters: {'n_estimators': 907, 'max_depth': 8, 'learning_rate': 0.026615811525635916, 'subsample': 0.7533218618753501, 'colsample_bytree': 0.8060942623788985, 'reg_alpha': 4.769278496749635e-06, 'reg_lambda': 0.07708297550190224, 'min_child_weight': 1}. Best is trial 27 with value: 0.7786842789109854.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 27. Best value: 0.778684:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 29/50 [20:45<17:26, 49.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 08:43:33,529] Trial 28 finished with value: 0.773891420197216 and parameters: {'n_estimators': 893, 'max_depth': 7, 'learning_rate': 0.026049816803947377, 'subsample': 0.7939275602754168, 'colsample_bytree': 0.8104853769236435, 'reg_alpha': 2.1151052225133375e-06, 'reg_lambda': 3.1069277755442704, 'min_child_weight': 4}. Best is trial 27 with value: 0.7786842789109854.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 27. Best value: 0.778684:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 30/50 [21:45<17:37, 52.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 08:44:33,530] Trial 29 finished with value: 0.7772565071436424 and parameters: {'n_estimators': 945, 'max_depth': 10, 'learning_rate': 0.016576874081028706, 'subsample': 0.7471317826282812, 'colsample_bytree': 0.6639704343300576, 'reg_alpha': 1.337021264610113e-05, 'reg_lambda': 0.15298340784458997, 'min_child_weight': 8}. Best is trial 27 with value: 0.7786842789109854.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 27. Best value: 0.778684:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 31/50 [22:42<17:08, 54.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 08:45:30,577] Trial 30 finished with value: 0.7746862312869975 and parameters: {'n_estimators': 921, 'max_depth': 10, 'learning_rate': 0.015799358853304964, 'subsample': 0.7525896139600842, 'colsample_bytree': 0.602034200592777, 'reg_alpha': 7.537494568497639e-07, 'reg_lambda': 0.1861440541875414, 'min_child_weight': 8}. Best is trial 27 with value: 0.7786842789109854.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 27. Best value: 0.778684:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 32/50 [23:38<16:19, 54.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 08:46:25,717] Trial 31 finished with value: 0.7760093178451043 and parameters: {'n_estimators': 944, 'max_depth': 9, 'learning_rate': 0.020308746480569028, 'subsample': 0.825723361608901, 'colsample_bytree': 0.6724497168095079, 'reg_alpha': 1.2902965703664927e-05, 'reg_lambda': 0.9308009321742825, 'min_child_weight': 8}. Best is trial 27 with value: 0.7786842789109854.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 27. Best value: 0.778684:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 33/50 [24:32<15:23, 54.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 08:47:19,741] Trial 32 finished with value: 0.7751730111898254 and parameters: {'n_estimators': 840, 'max_depth': 10, 'learning_rate': 0.013397848000633136, 'subsample': 0.7863865890559196, 'colsample_bytree': 0.6410437403279615, 'reg_alpha': 0.00013125454973358073, 'reg_lambda': 0.07671535159777253, 'min_child_weight': 10}. Best is trial 27 with value: 0.7786842789109854.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 27. Best value: 0.778684:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 34/50 [25:05<12:51, 48.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 08:47:53,697] Trial 33 finished with value: 0.7747546932725011 and parameters: {'n_estimators': 653, 'max_depth': 8, 'learning_rate': 0.029647520526342605, 'subsample': 0.7487801373250512, 'colsample_bytree': 0.7854592767528895, 'reg_alpha': 5.082806781797602e-06, 'reg_lambda': 0.0007616776168097366, 'min_child_weight': 9}. Best is trial 27 with value: 0.7786842789109854.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 27. Best value: 0.778684:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 35/50 [26:07<13:01, 52.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 08:48:54,949] Trial 34 finished with value: 0.7737691748450537 and parameters: {'n_estimators': 945, 'max_depth': 9, 'learning_rate': 0.02235576183484507, 'subsample': 0.7063390331915702, 'colsample_bytree': 0.8485896935001601, 'reg_alpha': 3.1788805252885283e-07, 'reg_lambda': 0.006443495086932719, 'min_child_weight': 7}. Best is trial 27 with value: 0.7786842789109854.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 27. Best value: 0.778684:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 36/50 [26:40<10:51, 46.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 08:49:28,471] Trial 35 finished with value: 0.7748624057345094 and parameters: {'n_estimators': 462, 'max_depth': 10, 'learning_rate': 0.018122942286201385, 'subsample': 0.8031575383246622, 'colsample_bytree': 0.7179640426499325, 'reg_alpha': 8.489409597477216e-07, 'reg_lambda': 7.912897193038097e-07, 'min_child_weight': 9}. Best is trial 27 with value: 0.7786842789109854.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 27. Best value: 0.778684:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 37/50 [27:30<10:18, 47.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 08:50:18,555] Trial 36 finished with value: 0.7731743772035686 and parameters: {'n_estimators': 857, 'max_depth': 7, 'learning_rate': 0.013488561180891149, 'subsample': 0.736112825656575, 'colsample_bytree': 0.8222199452738695, 'reg_alpha': 0.00012007755266826962, 'reg_lambda': 9.649800401875726, 'min_child_weight': 5}. Best is trial 27 with value: 0.7786842789109854.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 27. Best value: 0.778684:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 38/50 [28:16<09:25, 47.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 08:51:04,557] Trial 37 finished with value: 0.777125286607063 and parameters: {'n_estimators': 773, 'max_depth': 8, 'learning_rate': 0.034527296799349357, 'subsample': 0.6617506159252935, 'colsample_bytree': 0.7312202514997275, 'reg_alpha': 0.005685642400676428, 'reg_lambda': 0.6853029470577725, 'min_child_weight': 8}. Best is trial 27 with value: 0.7786842789109854.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 27. Best value: 0.778684:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 39/50 [28:42<07:28, 40.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 08:51:30,390] Trial 38 finished with value: 0.7709787815166006 and parameters: {'n_estimators': 579, 'max_depth': 6, 'learning_rate': 0.03453465237587204, 'subsample': 0.61115107840185, 'colsample_bytree': 0.6955057524588949, 'reg_alpha': 1.422988793648567e-05, 'reg_lambda': 0.8081015838073446, 'min_child_weight': 8}. Best is trial 27 with value: 0.7786842789109854.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 27. Best value: 0.778684:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 40/50 [29:09<06:05, 36.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 08:51:57,061] Trial 39 finished with value: 0.7718051415927668 and parameters: {'n_estimators': 995, 'max_depth': 5, 'learning_rate': 0.07701946104313157, 'subsample': 0.650395329750268, 'colsample_bytree': 0.6604394076572676, 'reg_alpha': 2.1355142183198276e-07, 'reg_lambda': 0.07648313476774964, 'min_child_weight': 9}. Best is trial 27 with value: 0.7786842789109854.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 27. Best value: 0.778684:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 41/50 [29:45<05:27, 36.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 08:52:33,322] Trial 40 finished with value: 0.7726948450425852 and parameters: {'n_estimators': 676, 'max_depth': 9, 'learning_rate': 0.04429091175691712, 'subsample': 0.7122711759429708, 'colsample_bytree': 0.73235910852764, 'reg_alpha': 2.322835082631229e-06, 'reg_lambda': 0.004466862399676087, 'min_child_weight': 6}. Best is trial 27 with value: 0.7786842789109854.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 27. Best value: 0.778684:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 42/50 [30:35<05:24, 40.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 08:53:23,618] Trial 41 finished with value: 0.7749418019677312 and parameters: {'n_estimators': 756, 'max_depth': 8, 'learning_rate': 0.02309259231833392, 'subsample': 0.6732505251156686, 'colsample_bytree': 0.7717980282121191, 'reg_alpha': 0.004770545910894634, 'reg_lambda': 0.3154595255710828, 'min_child_weight': 8}. Best is trial 27 with value: 0.7786842789109854.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 27. Best value: 0.778684:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 43/50 [31:22<04:57, 42.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 08:54:10,556] Trial 42 finished with value: 0.7777836232546471 and parameters: {'n_estimators': 770, 'max_depth': 7, 'learning_rate': 0.02404402743421803, 'subsample': 0.6291748922019295, 'colsample_bytree': 0.8540545370426669, 'reg_alpha': 0.003160239106756127, 'reg_lambda': 1.1184188181877631e-05, 'min_child_weight': 2}. Best is trial 27 with value: 0.7786842789109854.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 27. Best value: 0.778684:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 44/50 [32:16<04:34, 45.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 08:55:04,028] Trial 43 finished with value: 0.7724544419567022 and parameters: {'n_estimators': 794, 'max_depth': 7, 'learning_rate': 0.010221382185056855, 'subsample': 0.6236205411546546, 'colsample_bytree': 0.8544748662254888, 'reg_alpha': 0.11411631323777839, 'reg_lambda': 1.3235082439226452e-05, 'min_child_weight': 2}. Best is trial 27 with value: 0.7786842789109854.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 27. Best value: 0.778684:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 45/50 [32:38<03:13, 38.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 08:55:26,385] Trial 44 finished with value: 0.7679795105917228 and parameters: {'n_estimators': 288, 'max_depth': 7, 'learning_rate': 0.019143397993802692, 'subsample': 0.8227408852630139, 'colsample_bytree': 0.8268836850065772, 'reg_alpha': 0.011918981812919052, 'reg_lambda': 1.099862741113366e-06, 'min_child_weight': 2}. Best is trial 27 with value: 0.7786842789109854.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 27. Best value: 0.778684:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 46/50 [33:21<02:39, 39.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 08:56:09,045] Trial 45 finished with value: 0.775548259147959 and parameters: {'n_estimators': 872, 'max_depth': 6, 'learning_rate': 0.030074512467154, 'subsample': 0.7710577326469937, 'colsample_bytree': 0.7491915398623794, 'reg_alpha': 0.42451433963420726, 'reg_lambda': 1.0191612039916994e-05, 'min_child_weight': 1}. Best is trial 27 with value: 0.7786842789109854.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 27. Best value: 0.778684:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 47/50 [34:23<02:19, 46.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 08:57:11,090] Trial 46 finished with value: 0.7724040400584828 and parameters: {'n_estimators': 922, 'max_depth': 9, 'learning_rate': 0.036029062898483254, 'subsample': 0.9643309908483702, 'colsample_bytree': 0.7178415647000129, 'reg_alpha': 5.0356803568304966e-05, 'reg_lambda': 3.298999604888277e-05, 'min_child_weight': 2}. Best is trial 27 with value: 0.7786842789109854.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 27. Best value: 0.778684:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 48/50 [34:43<01:17, 38.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 08:57:31,280] Trial 47 finished with value: 0.7642149566104487 and parameters: {'n_estimators': 603, 'max_depth': 5, 'learning_rate': 0.013495599714784128, 'subsample': 0.6975696559640742, 'colsample_bytree': 0.8608166366510229, 'reg_alpha': 0.00013859928284339337, 'reg_lambda': 2.007296002988178, 'min_child_weight': 7}. Best is trial 27 with value: 0.7786842789109854.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 27. Best value: 0.778684:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 49/50 [35:12<00:35, 35.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 08:57:59,783] Trial 48 finished with value: 0.7731896339591323 and parameters: {'n_estimators': 685, 'max_depth': 7, 'learning_rate': 0.01792040002220024, 'subsample': 0.7320284378156228, 'colsample_bytree': 0.6362058647750269, 'reg_alpha': 0.003217255523390448, 'reg_lambda': 4.168526362623191e-06, 'min_child_weight': 10}. Best is trial 27 with value: 0.7786842789109854.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 27. Best value: 0.778684: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [36:45<00:00, 44.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 08:59:33,456] Trial 49 finished with value: 0.7753686972308497 and parameters: {'n_estimators': 769, 'max_depth': 9, 'learning_rate': 0.03092227170366627, 'subsample': 0.6052535552160798, 'colsample_bytree': 0.9050156164185065, 'reg_alpha': 4.926219861437245e-06, 'reg_lambda': 1.6079647197427119e-07, 'min_child_weight': 1}. Best is trial 27 with value: 0.7786842789109854.\n",
      "\n",
      "‚úÖ XGBoost optimization completed in 2205.77 seconds\n",
      "   Best F1 score: 0.7787\n",
      "   Best parameters found:\n",
      "     n_estimators: 907\n",
      "     max_depth: 8\n",
      "     learning_rate: 0.026615811525635916\n",
      "     subsample: 0.7533218618753501\n",
      "     colsample_bytree: 0.8060942623788985\n",
      "     reg_alpha: 4.769278496749635e-06\n",
      "     reg_lambda: 0.07708297550190224\n",
      "     min_child_weight: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def optimize_xgboost(X, y, groups, class_weights, n_trials=50):\n",
    "    \"\"\"\n",
    "    Find the best XGBoost hyperparameters using Optuna.\n",
    "    \n",
    "    Parameters:\n",
    "    - X: Features\n",
    "    - y: Labels\n",
    "    - groups: Star groups for cross-validation\n",
    "    - class_weights: Weights for imbalanced classes\n",
    "    - n_trials: Number of parameter combinations to try\n",
    "    \"\"\"\n",
    "    if not XGBOOST_AVAILABLE:\n",
    "        print(\"[X] XGBoost not available, skipping optimization\")\n",
    "        return None, None, 0\n",
    "        \n",
    "    print(\"Optimizing XGBoost Hyperparameters...\")\n",
    "    print(f\"   Will try {n_trials} different parameter combinations\")\n",
    "    \n",
    "    def objective(trial):\n",
    "        \"\"\"\n",
    "        This function defines what parameters Optuna should try.\n",
    "        Optuna will call this function many times with different parameter values.\n",
    "        \"\"\"\n",
    "        # Define the parameter search space\n",
    "        params = {\n",
    "            'objective': 'multi:softprob',  # Multi-class classification\n",
    "            'eval_metric': 'mlogloss',      # Loss function\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),  # Number of trees\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 10),             # Tree depth\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),  # Learning rate\n",
    "            'subsample': trial.suggest_float('subsample', 0.6, 1.0),        # Row sampling\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),  # Column sampling\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),    # L1 regularization\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),  # L2 regularization\n",
    "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),       # Min samples in leaf\n",
    "            'random_state': 42,  # For reproducible results\n",
    "            'verbosity': 0,      # Quiet output\n",
    "            'n_jobs': -1         # Use all CPU cores\n",
    "        }\n",
    "        \n",
    "        # Create sample weights for class imbalance\n",
    "        sample_weights = np.array([class_weights[label] for label in y])\n",
    "        \n",
    "        # Create the model\n",
    "        model = xgb.XGBClassifier(**params)\n",
    "        \n",
    "        # Evaluate using cross-validation\n",
    "        # GroupKFold ensures samples from the same star don't appear in both train and validation\n",
    "        cv = GroupKFold(n_splits=5)\n",
    "        scores = []\n",
    "        \n",
    "        for train_idx, val_idx in cv.split(X, y, groups):\n",
    "            X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n",
    "            y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "            sample_weights_fold = sample_weights[train_idx]\n",
    "            \n",
    "            try:\n",
    "                # Train on this fold\n",
    "                model.fit(X_train_fold, y_train_fold, sample_weight=sample_weights_fold)\n",
    "                # Predict on validation fold\n",
    "                y_pred_fold = model.predict(X_val_fold)\n",
    "                # Calculate F1 score (our target metric)\n",
    "                fold_score = f1_score(y_val_fold, y_pred_fold, average='macro')\n",
    "                scores.append(fold_score)\n",
    "            except Exception:\n",
    "                # If something goes wrong, return a bad score\n",
    "                return 0.0\n",
    "        \n",
    "        # Return the average F1 score across all folds\n",
    "        return np.mean(scores) if scores else 0.0\n",
    "    \n",
    "    # Create and run the optimization study\n",
    "    study = optuna.create_study(direction='maximize', sampler=TPESampler(seed=42))\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Run the optimization\n",
    "    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
    "    \n",
    "    optimization_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\nXGBoost optimization completed in {optimization_time:.2f} seconds\")\n",
    "    print(f\"   Best F1 score: {study.best_value:.4f}\")\n",
    "    print(f\"   Best parameters found:\")\n",
    "    for param, value in study.best_params.items():\n",
    "        print(f\"     {param}: {value}\")\n",
    "    \n",
    "    return study.best_params, study.best_value, optimization_time\n",
    "\n",
    "# Run XGBoost optimization (if available)\n",
    "if XGBOOST_AVAILABLE:\n",
    "    xgb_params, xgb_score, xgb_time = optimize_xgboost(X_enhanced, y, groups, class_weights, n_trials=50)\n",
    "else:\n",
    "    xgb_params, xgb_score, xgb_time = None, None, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6e0c35",
   "metadata": {},
   "source": [
    "### Optimizing LightGBM Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123eb392",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 09:00:59,491] A new study created in memory with name: no-name-262e6a41-edf3-4dbc-ad5b-37c5e7bab781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Optimizing LightGBM Hyperparameters...\n",
      "   Will try 50 different parameter combinations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.767003:   2%|‚ñè         | 1/50 [00:13<11:22, 13.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 09:01:13,414] Trial 0 finished with value: 0.7670034775888992 and parameters: {'n_estimators': 437, 'max_depth': 10, 'learning_rate': 0.1205712628744377, 'subsample': 0.8394633936788146, 'colsample_bytree': 0.6624074561769746, 'reg_alpha': 2.5348407664333426e-07, 'reg_lambda': 3.3323645788192616e-08, 'min_child_samples': 88, 'num_leaves': 184}. Best is trial 0 with value: 0.7670034775888992.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.767003:   4%|‚ñç         | 2/50 [00:19<07:01,  8.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 09:01:18,584] Trial 1 finished with value: 0.7565592904657242 and parameters: {'n_estimators': 737, 'max_depth': 3, 'learning_rate': 0.2708160864249968, 'subsample': 0.9329770563201687, 'colsample_bytree': 0.6849356442713105, 'reg_alpha': 4.329370014459266e-07, 'reg_lambda': 4.4734294104626844e-07, 'min_child_samples': 34, 'num_leaves': 162}. Best is trial 0 with value: 0.7670034775888992.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.770275:   6%|‚ñå         | 3/50 [00:25<06:07,  7.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 09:01:25,247] Trial 2 finished with value: 0.7702752424463613 and parameters: {'n_estimators': 489, 'max_depth': 5, 'learning_rate': 0.08012737503998542, 'subsample': 0.6557975442608167, 'colsample_bytree': 0.7168578594140873, 'reg_alpha': 1.9826980964985924e-05, 'reg_lambda': 0.00012724181576752517, 'min_child_samples': 80, 'num_leaves': 68}. Best is trial 2 with value: 0.7702752424463613.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.770275:   8%|‚ñä         | 4/50 [00:37<07:16,  9.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 09:01:37,285] Trial 3 finished with value: 0.7648994761608614 and parameters: {'n_estimators': 563, 'max_depth': 7, 'learning_rate': 0.011711509955524094, 'subsample': 0.8430179407605753, 'colsample_bytree': 0.6682096494749166, 'reg_alpha': 3.850031979199519e-08, 'reg_lambda': 3.4671276804481113, 'min_child_samples': 97, 'num_leaves': 245}. Best is trial 2 with value: 0.7702752424463613.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.770275:  10%|‚ñà         | 5/50 [00:41<05:36,  7.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 09:01:41,190] Trial 4 finished with value: 0.7616662872087712 and parameters: {'n_estimators': 374, 'max_depth': 3, 'learning_rate': 0.1024932221692416, 'subsample': 0.7760609974958406, 'colsample_bytree': 0.6488152939379115, 'reg_alpha': 0.00028614897264046574, 'reg_lambda': 2.039373116525212e-08, 'min_child_samples': 92, 'num_leaves': 85}. Best is trial 2 with value: 0.7702752424463613.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.770275:  12%|‚ñà‚ñè        | 6/50 [00:56<07:25, 10.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 09:01:56,464] Trial 5 finished with value: 0.7702412277700896 and parameters: {'n_estimators': 696, 'max_depth': 5, 'learning_rate': 0.05864129169696527, 'subsample': 0.8186841117373118, 'colsample_bytree': 0.6739417822102108, 'reg_alpha': 5.324289357128436, 'reg_lambda': 0.09466630153726856, 'min_child_samples': 95, 'num_leaves': 270}. Best is trial 2 with value: 0.7702752424463613.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 6. Best value: 0.774438:  14%|‚ñà‚ñç        | 7/50 [01:41<15:22, 21.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 09:02:41,252] Trial 6 finished with value: 0.7744380778121707 and parameters: {'n_estimators': 638, 'max_depth': 10, 'learning_rate': 0.01351182947645082, 'subsample': 0.6783931449676581, 'colsample_bytree': 0.6180909155642152, 'reg_alpha': 8.471746987003668e-06, 'reg_lambda': 3.148441347423712e-05, 'min_child_samples': 31, 'num_leaves': 251}. Best is trial 6 with value: 0.7744380778121707.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 6. Best value: 0.774438:  16%|‚ñà‚ñå        | 8/50 [01:50<12:05, 17.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 09:02:49,579] Trial 7 finished with value: 0.7669592353594062 and parameters: {'n_estimators': 421, 'max_depth': 5, 'learning_rate': 0.06333268775321842, 'subsample': 0.6563696899899051, 'colsample_bytree': 0.9208787923016158, 'reg_alpha': 4.6876566400928895e-08, 'reg_lambda': 7.620481786158549, 'min_child_samples': 79, 'num_leaves': 67}. Best is trial 6 with value: 0.7744380778121707.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 6. Best value: 0.774438:  18%|‚ñà‚ñä        | 9/50 [02:01<10:37, 15.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 09:03:01,345] Trial 8 finished with value: 0.7655705868215261 and parameters: {'n_estimators': 104, 'max_depth': 9, 'learning_rate': 0.11069143219393454, 'subsample': 0.8916028672163949, 'colsample_bytree': 0.9085081386743783, 'reg_alpha': 4.638759594322625e-08, 'reg_lambda': 1.683416412018213e-05, 'min_child_samples': 16, 'num_leaves': 261}. Best is trial 6 with value: 0.7744380778121707.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 6. Best value: 0.774438:  20%|‚ñà‚ñà        | 10/50 [02:14<09:41, 14.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 09:03:13,582] Trial 9 finished with value: 0.7609448632984824 and parameters: {'n_estimators': 661, 'max_depth': 5, 'learning_rate': 0.012413189635294229, 'subsample': 0.7243929286862649, 'colsample_bytree': 0.7300733288106989, 'reg_alpha': 0.036851536911881845, 'reg_lambda': 0.005470376807480391, 'min_child_samples': 90, 'num_leaves': 147}. Best is trial 6 with value: 0.7744380778121707.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 6. Best value: 0.774438:  22%|‚ñà‚ñà‚ñè       | 11/50 [03:00<15:50, 24.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 09:04:00,288] Trial 10 finished with value: 0.771328491682485 and parameters: {'n_estimators': 959, 'max_depth': 8, 'learning_rate': 0.02348130822266853, 'subsample': 0.6071847502459278, 'colsample_bytree': 0.8262452362725613, 'reg_alpha': 1.8026362255550884e-05, 'reg_lambda': 2.565335661250384e-06, 'min_child_samples': 42, 'num_leaves': 204}. Best is trial 6 with value: 0.7744380778121707.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 6. Best value: 0.774438:  24%|‚ñà‚ñà‚ñç       | 12/50 [03:53<20:54, 33.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 09:04:53,091] Trial 11 finished with value: 0.7682398871545815 and parameters: {'n_estimators': 997, 'max_depth': 8, 'learning_rate': 0.02304364042504696, 'subsample': 0.6105733875603306, 'colsample_bytree': 0.8222008072142497, 'reg_alpha': 2.2830581391957638e-05, 'reg_lambda': 5.830012810506096e-06, 'min_child_samples': 50, 'num_leaves': 215}. Best is trial 6 with value: 0.7744380778121707.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 6. Best value: 0.774438:  26%|‚ñà‚ñà‚ñå       | 13/50 [04:55<25:41, 41.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 09:05:54,662] Trial 12 finished with value: 0.7681279859931027 and parameters: {'n_estimators': 955, 'max_depth': 10, 'learning_rate': 0.027540815560541373, 'subsample': 0.602822975090678, 'colsample_bytree': 0.8027936026422099, 'reg_alpha': 1.5155324681197734e-05, 'reg_lambda': 0.0007187551363037014, 'min_child_samples': 36, 'num_leaves': 298}. Best is trial 6 with value: 0.7744380778121707.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 6. Best value: 0.774438:  28%|‚ñà‚ñà‚ñä       | 14/50 [05:49<27:15, 45.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 09:06:48,735] Trial 13 finished with value: 0.7656462938344134 and parameters: {'n_estimators': 867, 'max_depth': 8, 'learning_rate': 0.02546406023240791, 'subsample': 0.72188135635799, 'colsample_bytree': 0.8489567844781626, 'reg_alpha': 0.0053735754500927975, 'reg_lambda': 1.492775149980716e-06, 'min_child_samples': 11, 'num_leaves': 209}. Best is trial 6 with value: 0.7744380778121707.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 6. Best value: 0.774438:  30%|‚ñà‚ñà‚ñà       | 15/50 [05:59<20:18, 34.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 09:06:59,019] Trial 14 finished with value: 0.7677907902701857 and parameters: {'n_estimators': 241, 'max_depth': 9, 'learning_rate': 0.017701336014162388, 'subsample': 0.9984067957444854, 'colsample_bytree': 0.9982518431502138, 'reg_alpha': 2.0660678072091953e-06, 'reg_lambda': 0.00010126904542105301, 'min_child_samples': 62, 'num_leaves': 123}. Best is trial 6 with value: 0.7744380778121707.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 6. Best value: 0.774438:  32%|‚ñà‚ñà‚ñà‚ñè      | 16/50 [06:29<18:50, 33.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 09:07:28,627] Trial 15 finished with value: 0.7713506598657234 and parameters: {'n_estimators': 845, 'max_depth': 7, 'learning_rate': 0.03423974663212451, 'subsample': 0.6863462441573819, 'colsample_bytree': 0.7664815103084881, 'reg_alpha': 0.0004814040252448574, 'reg_lambda': 5.145599919643514e-07, 'min_child_samples': 26, 'num_leaves': 225}. Best is trial 6 with value: 0.7744380778121707.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 6. Best value: 0.774438:  34%|‚ñà‚ñà‚ñà‚ñç      | 17/50 [06:42<15:00, 27.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 09:07:42,046] Trial 16 finished with value: 0.7695734359341331 and parameters: {'n_estimators': 827, 'max_depth': 6, 'learning_rate': 0.03748251495486013, 'subsample': 0.7290460798152136, 'colsample_bytree': 0.6024625741317283, 'reg_alpha': 0.0015579107174789573, 'reg_lambda': 1.426897615591071e-07, 'min_child_samples': 24, 'num_leaves': 13}. Best is trial 6 with value: 0.7744380778121707.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 6. Best value: 0.774438:  36%|‚ñà‚ñà‚ñà‚ñå      | 18/50 [07:00<13:03, 24.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 09:08:00,007] Trial 17 finished with value: 0.7730297592350104 and parameters: {'n_estimators': 570, 'max_depth': 7, 'learning_rate': 0.04222070198361061, 'subsample': 0.6748397462452165, 'colsample_bytree': 0.7557193275020625, 'reg_alpha': 0.14551139103143435, 'reg_lambda': 0.0044338238945435225, 'min_child_samples': 26, 'num_leaves': 300}. Best is trial 6 with value: 0.7744380778121707.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 6. Best value: 0.774438:  38%|‚ñà‚ñà‚ñà‚ñä      | 19/50 [07:09<10:12, 19.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 09:08:08,752] Trial 18 finished with value: 0.7640739276496967 and parameters: {'n_estimators': 613, 'max_depth': 9, 'learning_rate': 0.20294436065143542, 'subsample': 0.7730982444721965, 'colsample_bytree': 0.6057424521763275, 'reg_alpha': 0.4450750227692883, 'reg_lambda': 0.02269399423925667, 'min_child_samples': 57, 'num_leaves': 297}. Best is trial 6 with value: 0.7744380778121707.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 6. Best value: 0.774438:  40%|‚ñà‚ñà‚ñà‚ñà      | 20/50 [07:18<08:21, 16.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 09:08:18,353] Trial 19 finished with value: 0.7708811788525772 and parameters: {'n_estimators': 320, 'max_depth': 6, 'learning_rate': 0.04181302194444169, 'subsample': 0.6779033211059171, 'colsample_bytree': 0.764088169035724, 'reg_alpha': 0.078255234974005, 'reg_lambda': 0.3053647726249895, 'min_child_samples': 21, 'num_leaves': 270}. Best is trial 6 with value: 0.7744380778121707.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 6. Best value: 0.774438:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 21/50 [07:25<06:40, 13.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 09:08:25,424] Trial 20 finished with value: 0.7602041378608252 and parameters: {'n_estimators': 500, 'max_depth': 4, 'learning_rate': 0.016501479505930235, 'subsample': 0.7570391886067509, 'colsample_bytree': 0.8741812449665195, 'reg_alpha': 1.3538798263084777, 'reg_lambda': 0.0012489084057036224, 'min_child_samples': 6, 'num_leaves': 243}. Best is trial 6 with value: 0.7744380778121707.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 6. Best value: 0.774438:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 22/50 [07:46<07:20, 15.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 09:08:45,608] Trial 21 finished with value: 0.7690245784001083 and parameters: {'n_estimators': 798, 'max_depth': 7, 'learning_rate': 0.04212358326121373, 'subsample': 0.6876708296465669, 'colsample_bytree': 0.7642614522042193, 'reg_alpha': 0.00018705875606506297, 'reg_lambda': 2.7760539148964445e-05, 'min_child_samples': 31, 'num_leaves': 231}. Best is trial 6 with value: 0.7744380778121707.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 6. Best value: 0.774438:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 23/50 [08:05<07:31, 16.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 09:09:04,616] Trial 22 finished with value: 0.7730564398855156 and parameters: {'n_estimators': 607, 'max_depth': 7, 'learning_rate': 0.03197470386409183, 'subsample': 0.6466552250225237, 'colsample_bytree': 0.7658313612926888, 'reg_alpha': 0.005558122254999426, 'reg_lambda': 2.916036278150057e-07, 'min_child_samples': 45, 'num_leaves': 277}. Best is trial 6 with value: 0.7744380778121707.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 6. Best value: 0.774438:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 24/50 [08:18<06:48, 15.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 09:09:18,008] Trial 23 finished with value: 0.7660524091830859 and parameters: {'n_estimators': 613, 'max_depth': 6, 'learning_rate': 0.014060774168081531, 'subsample': 0.6457530001147498, 'colsample_bytree': 0.7242705656418438, 'reg_alpha': 0.020479630342822033, 'reg_lambda': 0.0012196971236593922, 'min_child_samples': 42, 'num_leaves': 288}. Best is trial 6 with value: 0.7744380778121707.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 6. Best value: 0.774438:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 25/50 [08:40<07:20, 17.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 09:09:40,029] Trial 24 finished with value: 0.7696179463696387 and parameters: {'n_estimators': 550, 'max_depth': 8, 'learning_rate': 0.01032423432640871, 'subsample': 0.6383810991626553, 'colsample_bytree': 0.779949916825584, 'reg_alpha': 0.18500777955653455, 'reg_lambda': 0.005122552100304125, 'min_child_samples': 48, 'num_leaves': 263}. Best is trial 6 with value: 0.7744380778121707.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 6. Best value: 0.774438:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 26/50 [09:10<08:28, 21.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 09:10:09,608] Trial 25 finished with value: 0.7733250613418556 and parameters: {'n_estimators': 738, 'max_depth': 10, 'learning_rate': 0.018791507889709853, 'subsample': 0.6990192029710519, 'colsample_bytree': 0.7106132862016324, 'reg_alpha': 0.008133728167983013, 'reg_lambda': 3.292009938653875e-05, 'min_child_samples': 65, 'num_leaves': 281}. Best is trial 6 with value: 0.7744380778121707.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 6. Best value: 0.774438:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 27/50 [09:39<09:00, 23.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 09:10:38,533] Trial 26 finished with value: 0.7736423581309777 and parameters: {'n_estimators': 758, 'max_depth': 10, 'learning_rate': 0.017679787100618936, 'subsample': 0.7298810162629739, 'colsample_bytree': 0.6381282564615647, 'reg_alpha': 0.006647824591350842, 'reg_lambda': 2.9445798539560644e-05, 'min_child_samples': 66, 'num_leaves': 176}. Best is trial 6 with value: 0.7744380778121707.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 6. Best value: 0.774438:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 28/50 [10:11<09:39, 26.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 09:11:11,443] Trial 27 finished with value: 0.774107959024895 and parameters: {'n_estimators': 733, 'max_depth': 10, 'learning_rate': 0.01833166287322091, 'subsample': 0.7099059188737812, 'colsample_bytree': 0.6382300385859311, 'reg_alpha': 9.212514599899751e-05, 'reg_lambda': 3.595623491773141e-05, 'min_child_samples': 67, 'num_leaves': 187}. Best is trial 6 with value: 0.7744380778121707.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 6. Best value: 0.774438:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 29/50 [10:43<09:43, 27.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 09:11:42,627] Trial 28 finished with value: 0.7723469561944312 and parameters: {'n_estimators': 741, 'max_depth': 10, 'learning_rate': 0.014305384370658997, 'subsample': 0.7512320651959548, 'colsample_bytree': 0.6306254407670495, 'reg_alpha': 2.8303961696403006e-06, 'reg_lambda': 0.00010461392327457106, 'min_child_samples': 72, 'num_leaves': 184}. Best is trial 6 with value: 0.7744380778121707.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 6. Best value: 0.774438:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 30/50 [11:11<09:20, 28.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 09:12:11,210] Trial 29 finished with value: 0.7737006548554454 and parameters: {'n_estimators': 899, 'max_depth': 9, 'learning_rate': 0.019952409263246613, 'subsample': 0.8001127692810024, 'colsample_bytree': 0.6278695886001305, 'reg_alpha': 9.48214713993962e-05, 'reg_lambda': 6.491177588876859e-06, 'min_child_samples': 74, 'num_leaves': 184}. Best is trial 6 with value: 0.7744380778121707.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 6. Best value: 0.774438:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 31/50 [11:31<08:07, 25.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 09:12:31,264] Trial 30 finished with value: 0.7683269320412267 and parameters: {'n_estimators': 902, 'max_depth': 9, 'learning_rate': 0.010402283115860132, 'subsample': 0.8595849191581006, 'colsample_bytree': 0.625494446583074, 'reg_alpha': 0.0001288370507539796, 'reg_lambda': 6.585617321274071e-08, 'min_child_samples': 82, 'num_leaves': 135}. Best is trial 6 with value: 0.7744380778121707.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 6. Best value: 0.774438:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 32/50 [12:06<08:28, 28.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 09:13:05,554] Trial 31 finished with value: 0.77338589991364 and parameters: {'n_estimators': 795, 'max_depth': 10, 'learning_rate': 0.018142387521066353, 'subsample': 0.8031767914716814, 'colsample_bytree': 0.6397246373659531, 'reg_alpha': 0.0011166529825834878, 'reg_lambda': 5.890800909786524e-06, 'min_child_samples': 66, 'num_leaves': 179}. Best is trial 6 with value: 0.7744380778121707.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 6. Best value: 0.774438:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 33/50 [13:10<11:03, 39.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 09:14:09,784] Trial 32 finished with value: 0.7720595194792805 and parameters: {'n_estimators': 686, 'max_depth': 10, 'learning_rate': 0.0205341904932422, 'subsample': 0.7126269592367563, 'colsample_bytree': 0.6885649206471307, 'reg_alpha': 6.609317633115833e-05, 'reg_lambda': 1.1428965092161362e-05, 'min_child_samples': 73, 'num_leaves': 161}. Best is trial 6 with value: 0.7744380778121707.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 6. Best value: 0.774438:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 34/50 [13:50<10:32, 39.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 09:14:50,406] Trial 33 finished with value: 0.771919607870364 and parameters: {'n_estimators': 758, 'max_depth': 9, 'learning_rate': 0.015361341437371443, 'subsample': 0.748319511318388, 'colsample_bytree': 0.6936517745357946, 'reg_alpha': 3.1856944868798415e-06, 'reg_lambda': 0.0003581858369909185, 'min_child_samples': 56, 'num_leaves': 196}. Best is trial 6 with value: 0.7744380778121707.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 6. Best value: 0.774438:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 35/50 [14:20<09:09, 36.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 09:15:20,417] Trial 34 finished with value: 0.770371839225088 and parameters: {'n_estimators': 893, 'max_depth': 9, 'learning_rate': 0.02819974764295923, 'subsample': 0.7856643247236639, 'colsample_bytree': 0.6558824041120856, 'reg_alpha': 0.0013223627935363901, 'reg_lambda': 1.2304412750887254e-06, 'min_child_samples': 72, 'num_leaves': 114}. Best is trial 6 with value: 0.7744380778121707.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 6. Best value: 0.774438:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 36/50 [14:41<07:25, 31.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 09:15:41,016] Trial 35 finished with value: 0.7697938762019388 and parameters: {'n_estimators': 665, 'max_depth': 10, 'learning_rate': 0.01401753574035082, 'subsample': 0.8156142507990782, 'colsample_bytree': 0.6010671845109126, 'reg_alpha': 1.9550226019547987e-07, 'reg_lambda': 6.950467290462166e-05, 'min_child_samples': 85, 'num_leaves': 173}. Best is trial 6 with value: 0.7744380778121707.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 6. Best value: 0.774438:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 37/50 [15:12<06:50, 31.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 09:16:11,909] Trial 36 finished with value: 0.7646285614450704 and parameters: {'n_estimators': 786, 'max_depth': 10, 'learning_rate': 0.1475279834159676, 'subsample': 0.8465913353889284, 'colsample_bytree': 0.6563664891152383, 'reg_alpha': 3.871740071875449e-05, 'reg_lambda': 1.0925032175320941e-08, 'min_child_samples': 60, 'num_leaves': 150}. Best is trial 6 with value: 0.7744380778121707.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 6. Best value: 0.774438:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 38/50 [15:47<06:30, 32.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 09:16:46,856] Trial 37 finished with value: 0.7717593934403313 and parameters: {'n_estimators': 929, 'max_depth': 9, 'learning_rate': 0.012158736231090518, 'subsample': 0.8803716974226267, 'colsample_bytree': 0.6278322006043399, 'reg_alpha': 7.400737884968978e-07, 'reg_lambda': 4.062840650459587e-06, 'min_child_samples': 77, 'num_leaves': 192}. Best is trial 6 with value: 0.7744380778121707.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 6. Best value: 0.774438:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 39/50 [16:06<05:12, 28.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 09:17:05,639] Trial 38 finished with value: 0.7652761944878843 and parameters: {'n_estimators': 510, 'max_depth': 9, 'learning_rate': 0.07419134494139497, 'subsample': 0.7353560665671851, 'colsample_bytree': 0.6739396766286271, 'reg_alpha': 7.504484500470109e-06, 'reg_lambda': 3.5557418128337684e-05, 'min_child_samples': 67, 'num_leaves': 102}. Best is trial 6 with value: 0.7744380778121707.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 6. Best value: 0.774438:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 40/50 [16:43<05:10, 31.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 09:17:42,758] Trial 39 finished with value: 0.7710835726610468 and parameters: {'n_estimators': 707, 'max_depth': 10, 'learning_rate': 0.020293529677139776, 'subsample': 0.707218036282599, 'colsample_bytree': 0.6947774678952219, 'reg_alpha': 0.000442017896406312, 'reg_lambda': 0.0002643759549749079, 'min_child_samples': 98, 'num_leaves': 245}. Best is trial 6 with value: 0.7744380778121707.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 6. Best value: 0.774438:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 41/50 [17:08<04:24, 29.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 09:18:08,142] Trial 40 finished with value: 0.767820302473875 and parameters: {'n_estimators': 434, 'max_depth': 10, 'learning_rate': 0.05198302781935727, 'subsample': 0.7850996448628529, 'colsample_bytree': 0.6213473734439143, 'reg_alpha': 8.225535726189789e-05, 'reg_lambda': 1.2038068899282529e-05, 'min_child_samples': 53, 'num_leaves': 169}. Best is trial 6 with value: 0.7744380778121707.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 41. Best value: 0.775181:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 42/50 [17:35<03:48, 28.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 09:18:35,071] Trial 41 finished with value: 0.7751811176763478 and parameters: {'n_estimators': 787, 'max_depth': 10, 'learning_rate': 0.01748786618188582, 'subsample': 0.7727375823014725, 'colsample_bytree': 0.6463667045647261, 'reg_alpha': 0.0023742308135301276, 'reg_lambda': 9.46549575294289e-07, 'min_child_samples': 68, 'num_leaves': 178}. Best is trial 41 with value: 0.7751811176763478.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 41. Best value: 0.775181:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 43/50 [18:02<03:17, 28.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 09:19:02,268] Trial 42 finished with value: 0.7728814071779422 and parameters: {'n_estimators': 839, 'max_depth': 10, 'learning_rate': 0.02152400691127237, 'subsample': 0.7690120017810149, 'colsample_bytree': 0.6524220616362197, 'reg_alpha': 0.0028774692325457146, 'reg_lambda': 1.7965951998204259e-06, 'min_child_samples': 71, 'num_leaves': 219}. Best is trial 41 with value: 0.7751811176763478.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 41. Best value: 0.775181:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 44/50 [18:25<02:39, 26.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 09:19:24,835] Trial 43 finished with value: 0.7699281107340117 and parameters: {'n_estimators': 714, 'max_depth': 9, 'learning_rate': 0.013361602231966248, 'subsample': 0.8229290777214511, 'colsample_bytree': 0.6378576187747722, 'reg_alpha': 0.024246743099592814, 'reg_lambda': 6.239145719871612e-07, 'min_child_samples': 77, 'num_leaves': 138}. Best is trial 41 with value: 0.7751811176763478.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 41. Best value: 0.775181:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 45/50 [18:44<02:02, 24.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 09:19:44,459] Trial 44 finished with value: 0.7689080986856147 and parameters: {'n_estimators': 642, 'max_depth': 8, 'learning_rate': 0.016299057565191406, 'subsample': 0.6650365609432259, 'colsample_bytree': 0.6738095716852754, 'reg_alpha': 0.0005135959706507336, 'reg_lambda': 0.0002726846921498429, 'min_child_samples': 83, 'num_leaves': 199}. Best is trial 41 with value: 0.7751811176763478.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 41. Best value: 0.775181:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 46/50 [19:18<01:49, 27.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 09:20:18,483] Trial 45 finished with value: 0.7705770128440507 and parameters: {'n_estimators': 777, 'max_depth': 10, 'learning_rate': 0.024913145338318902, 'subsample': 0.7970806102251798, 'colsample_bytree': 0.6645719622975886, 'reg_alpha': 1.0974733069709799e-05, 'reg_lambda': 1.1144438078720906e-05, 'min_child_samples': 87, 'num_leaves': 159}. Best is trial 41 with value: 0.7751811176763478.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 41. Best value: 0.775181:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 47/50 [19:51<01:26, 28.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 09:20:51,035] Trial 46 finished with value: 0.7707111817044613 and parameters: {'n_estimators': 875, 'max_depth': 9, 'learning_rate': 0.029668607223706, 'subsample': 0.7472974225950906, 'colsample_bytree': 0.614316728017922, 'reg_alpha': 0.00022641522617742953, 'reg_lambda': 5.469635513843185e-08, 'min_child_samples': 60, 'num_leaves': 235}. Best is trial 41 with value: 0.7751811176763478.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 41. Best value: 0.775181:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 48/50 [20:32<01:04, 32.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 09:21:31,534] Trial 47 finished with value: 0.7724686803097577 and parameters: {'n_estimators': 997, 'max_depth': 10, 'learning_rate': 0.010907694678101569, 'subsample': 0.7008416850776104, 'colsample_bytree': 0.7415630707609756, 'reg_alpha': 3.803822586091271e-05, 'reg_lambda': 1.7260306706331124e-07, 'min_child_samples': 68, 'num_leaves': 208}. Best is trial 41 with value: 0.7751811176763478.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 41. Best value: 0.775181:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 49/50 [20:57<00:30, 30.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 09:21:57,397] Trial 48 finished with value: 0.7678541424646588 and parameters: {'n_estimators': 937, 'max_depth': 8, 'learning_rate': 0.012283482817550501, 'subsample': 0.9437156138026376, 'colsample_bytree': 0.7026475193483491, 'reg_alpha': 0.012328771463494346, 'reg_lambda': 4.3523570054742935e-05, 'min_child_samples': 92, 'num_leaves': 187}. Best is trial 41 with value: 0.7751811176763478.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 41. Best value: 0.775181: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [21:42<00:00, 26.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-05 09:22:42,400] Trial 49 finished with value: 0.7712752837622466 and parameters: {'n_estimators': 814, 'max_depth': 9, 'learning_rate': 0.023411420205229867, 'subsample': 0.6239537880636523, 'colsample_bytree': 0.6518550039009339, 'reg_alpha': 5.5075003993760586e-06, 'reg_lambda': 3.674853924659519e-06, 'min_child_samples': 35, 'num_leaves': 255}. Best is trial 41 with value: 0.7751811176763478.\n",
      "\n",
      "‚úÖ LightGBM optimization completed in 1302.91 seconds\n",
      "   Best F1 score: 0.7752\n",
      "   Best parameters found:\n",
      "     n_estimators: 787\n",
      "     max_depth: 10\n",
      "     learning_rate: 0.01748786618188582\n",
      "     subsample: 0.7727375823014725\n",
      "     colsample_bytree: 0.6463667045647261\n",
      "     reg_alpha: 0.0023742308135301276\n",
      "     reg_lambda: 9.46549575294289e-07\n",
      "     min_child_samples: 68\n",
      "     num_leaves: 178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def optimize_lightgbm(X, y, groups, n_trials=50):\n",
    "    \"\"\"\n",
    "    Find the best LightGBM hyperparameters using Optuna.\n",
    "    \"\"\"\n",
    "    if not LIGHTGBM_AVAILABLE:\n",
    "        print(\"LightGBM not available, skipping optimization\")\n",
    "        return None, None, 0\n",
    "        \n",
    "    print(\"Optimizing LightGBM Hyperparameters...\")\n",
    "    print(f\"   Will try {n_trials} different parameter combinations\")\n",
    "    \n",
    "    def objective(trial):\n",
    "        \"\"\"\n",
    "        Define LightGBM parameter search space.\n",
    "        LightGBM has some different parameters compared to XGBoost.\n",
    "        \"\"\"\n",
    "        params = {\n",
    "            'objective': 'multiclass',      # Multi-class classification\n",
    "            'metric': 'multi_logloss',      # Loss function\n",
    "            'boosting_type': 'gbdt',        # Gradient Boosting Decision Tree\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "            'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
    "            'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 10, 300),  # LightGBM-specific\n",
    "            'class_weight': 'balanced',     # Handle class imbalance automatically\n",
    "            'random_state': 42,\n",
    "            'verbosity': -1,    # Quiet output\n",
    "            'n_jobs': -1\n",
    "        }\n",
    "        \n",
    "        model = lgb.LGBMClassifier(**params)\n",
    "        \n",
    "        # LightGBM has better scikit-learn integration, so we can use cross_val_score directly\n",
    "        cv = GroupKFold(n_splits=5)\n",
    "        scores = cross_val_score(model, X, y, groups=groups, cv=cv, scoring='f1_macro', n_jobs=1)\n",
    "        \n",
    "        return scores.mean()\n",
    "    \n",
    "    # Create and run optimization\n",
    "    study = optuna.create_study(direction='maximize', sampler=TPESampler(seed=42))\n",
    "    start_time = time.time()\n",
    "    \n",
    "    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
    "    \n",
    "    optimization_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\nLightGBM optimization completed in {optimization_time:.2f} seconds\")\n",
    "    print(f\"   Best F1 score: {study.best_value:.4f}\")\n",
    "    print(f\"   Best parameters found:\")\n",
    "    for param, value in study.best_params.items():\n",
    "        print(f\"     {param}: {value}\")\n",
    "    \n",
    "    return study.best_params, study.best_value, optimization_time\n",
    "\n",
    "# Run LightGBM optimization (if available)\n",
    "if LIGHTGBM_AVAILABLE:\n",
    "    lgb_params, lgb_score, lgb_time = optimize_lightgbm(X_enhanced, y, groups, n_trials=50)\n",
    "else:\n",
    "    lgb_params, lgb_score, lgb_time = None, None, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc937dd8",
   "metadata": {},
   "source": [
    "## 6. Training Final Optimized Models\n",
    "\n",
    "Now that we have the best hyperparameters, let's train our final models and evaluate them properly using cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25974b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéì Training Final Optimized Models...\n",
      "\n",
      "üìä Training final XGBoost model...\n",
      "   Performing 5-fold cross-validation...\n",
      "     Training fold 1/5... F1: 0.7772\n",
      "     Training fold 2/5... F1: 0.7772\n",
      "     Training fold 2/5... F1: 0.7921\n",
      "     Training fold 3/5... F1: 0.7921\n",
      "     Training fold 3/5... F1: 0.7758\n",
      "     Training fold 4/5... F1: 0.7758\n",
      "     Training fold 4/5... F1: 0.7816\n",
      "     Training fold 5/5... F1: 0.7816\n",
      "     Training fold 5/5... F1: 0.7668\n",
      "   Training on full dataset...\n",
      "F1: 0.7668\n",
      "   Training on full dataset...\n",
      "   ‚úÖ XGBoost Final Score: 0.7787 ¬± 0.0083\n",
      "\n",
      "üìä Training final LightGBM model...\n",
      "   Performing 5-fold cross-validation...\n",
      "   ‚úÖ XGBoost Final Score: 0.7787 ¬± 0.0083\n",
      "\n",
      "üìä Training final LightGBM model...\n",
      "   Performing 5-fold cross-validation...\n",
      "   Training on full dataset...\n",
      "   Training on full dataset...\n",
      "   ‚úÖ LightGBM Final Score: 0.7662 ¬± 0.0078\n",
      "   ‚úÖ LightGBM Final Score: 0.7662 ¬± 0.0078\n"
     ]
    }
   ],
   "source": [
    "def train_final_models(X, y, groups, xgb_params, lgb_params, class_weights):\n",
    "    \"\"\"\n",
    "    Train the final optimized models and evaluate them with cross-validation.\n",
    "    \"\"\"\n",
    "    print(\"Training Final Optimized Models...\")\n",
    "    \n",
    "    cv = GroupKFold(n_splits=5)\n",
    "    final_results = {}\n",
    "    \n",
    "    # Train final XGBoost model\n",
    "    if XGBOOST_AVAILABLE and xgb_params:\n",
    "        print(\"\\nTraining final XGBoost model...\")\n",
    "        \n",
    "        # Create sample weights for XGBoost\n",
    "        sample_weights = np.array([class_weights[label] for label in y])\n",
    "        \n",
    "        xgb_model = xgb.XGBClassifier(\n",
    "            random_state=42,\n",
    "            verbosity=0,\n",
    "            n_jobs=-1,\n",
    "            **xgb_params\n",
    "        )\n",
    "        \n",
    "        # Perform cross-validation to get unbiased performance estimate\n",
    "        scores = []\n",
    "        y_true_all, y_pred_all = [], []\n",
    "        \n",
    "        print(\"   Performing 5-fold cross-validation...\")\n",
    "        for fold_num, (train_idx, val_idx) in enumerate(cv.split(X, y, groups), 1):\n",
    "            print(f\"     Training fold {fold_num}/5...\", end=\" \")\n",
    "            \n",
    "            X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n",
    "            y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "            sample_weights_fold = sample_weights[train_idx]\n",
    "            \n",
    "            # Train on this fold\n",
    "            xgb_model.fit(X_train_fold, y_train_fold, sample_weight=sample_weights_fold)\n",
    "            # Predict on validation set\n",
    "            y_pred_fold = xgb_model.predict(X_val_fold)\n",
    "            \n",
    "            # Calculate F1 score for this fold\n",
    "            fold_score = f1_score(y_val_fold, y_pred_fold, average='macro')\n",
    "            scores.append(fold_score)\n",
    "            print(f\"F1: {fold_score:.4f}\")\n",
    "            \n",
    "            # Collect all predictions for detailed analysis\n",
    "            y_true_all.extend(y_val_fold)\n",
    "            y_pred_all.extend(y_pred_fold)\n",
    "        \n",
    "        # Train final model on full dataset\n",
    "        print(\"   Training on full dataset...\")\n",
    "        xgb_model.fit(X, y, sample_weight=sample_weights)\n",
    "        \n",
    "        # Store results\n",
    "        final_results['xgboost'] = {\n",
    "            'model': xgb_model,\n",
    "            'cv_scores': scores,\n",
    "            'mean_cv_score': np.mean(scores),\n",
    "            'std_cv_score': np.std(scores),\n",
    "            'per_class_f1': f1_score(y_true_all, y_pred_all, average=None),\n",
    "            'params': xgb_params\n",
    "        }\n",
    "        \n",
    "        print(f\"   XGBoost Final Score: {np.mean(scores):.4f} ¬± {np.std(scores):.4f}\")\n",
    "    \n",
    "    # Train final LightGBM model\n",
    "    if LIGHTGBM_AVAILABLE and lgb_params:\n",
    "        print(\"\\nTraining final LightGBM model...\")\n",
    "        \n",
    "        lgb_model = lgb.LGBMClassifier(\n",
    "            random_state=42,\n",
    "            verbosity=-1,\n",
    "            n_jobs=-1,\n",
    "            **lgb_params\n",
    "        )\n",
    "        \n",
    "        print(\"   Performing 5-fold cross-validation...\")\n",
    "        scores = cross_val_score(lgb_model, X, y, groups=groups, cv=cv, scoring='f1_macro', n_jobs=1)\n",
    "        \n",
    "        # Train final model on full dataset\n",
    "        print(\"   Training on full dataset...\")\n",
    "        lgb_model.fit(X, y)\n",
    "        \n",
    "        final_results['lightgbm'] = {\n",
    "            'model': lgb_model,\n",
    "            'cv_scores': scores,\n",
    "            'mean_cv_score': scores.mean(),\n",
    "            'std_cv_score': scores.std(),\n",
    "            'params': lgb_params\n",
    "        }\n",
    "        \n",
    "        print(f\"   LightGBM Final Score: {scores.mean():.4f} ¬± {scores.std():.4f}\")\n",
    "    \n",
    "    return final_results\n",
    "\n",
    "# Train final models\n",
    "final_results = train_final_models(X_enhanced, y, groups, xgb_params, lgb_params, class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315f4f7c",
   "metadata": {},
   "source": [
    "## 7. Model Ensemble\n",
    "\n",
    "**Ensemble methods** combine multiple models to create a stronger predictor. The idea is that different models might make different types of errors, so combining them can reduce overall error.\n",
    "\n",
    "### Why Ensemble Works:\n",
    "- **Diversity**: Different algorithms have different strengths\n",
    "- **Noise Reduction**: Random errors tend to cancel out\n",
    "- **Better Generalization**: Less likely to overfit to specific patterns\n",
    "\n",
    "We'll create a simple **averaging ensemble** where we average the predictions from both XGBoost and LightGBM.\n",
    "\n",
    "--> future: make an equation that will help make the predictions better instead of just averaging em!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0d71523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Gradient Boosting Ensemble...\n",
      "   Strategy: Average the predicted probabilities from both models\n",
      "\n",
      "   Evaluating ensemble with cross-validation:\n",
      "     Fold 1/5... F1: 0.7748\n",
      "     Fold 2/5... F1: 0.7748\n",
      "     Fold 2/5... F1: 0.7864\n",
      "     Fold 3/5... F1: 0.7864\n",
      "     Fold 3/5... F1: 0.7775\n",
      "     Fold 4/5... F1: 0.7775\n",
      "     Fold 4/5... F1: 0.7762\n",
      "     Fold 5/5... F1: 0.7762\n",
      "     Fold 5/5... F1: 0.7607\n",
      "\n",
      "   Ensemble Final Score: 0.7751 ¬± 0.0083\n",
      "F1: 0.7607\n",
      "\n",
      "   Ensemble Final Score: 0.7751 ¬± 0.0083\n"
     ]
    }
   ],
   "source": [
    "def create_ensemble(final_results, X, y, groups):\n",
    "    \"\"\"\n",
    "    Create an ensemble of our gradient boosted models.\n",
    "    \"\"\"\n",
    "    if len(final_results) < 2:\n",
    "        print(\"Not enough models for ensemble, returning best single model\")\n",
    "        if final_results:\n",
    "            best_model = max(final_results.items(), key=lambda x: x[1]['mean_cv_score'])\n",
    "            return best_model[1]\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    print(\"Creating Gradient Boosting Ensemble...\")\n",
    "    print(\"   Strategy: Average the predicted probabilities from both models\")\n",
    "    \n",
    "    # Evaluate ensemble using cross-validation\n",
    "    cv = GroupKFold(n_splits=5)\n",
    "    ensemble_scores = []\n",
    "    \n",
    "    print(\"\\n   Evaluating ensemble with cross-validation:\")\n",
    "    for fold_num, (train_idx, val_idx) in enumerate(cv.split(X, y, groups), 1):\n",
    "        print(f\"     Fold {fold_num}/5...\", end=\" \")\n",
    "        \n",
    "        X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        predictions = []\n",
    "        \n",
    "        # Get predictions from each model\n",
    "        for model_name, model_data in final_results.items():\n",
    "            model = model_data['model']\n",
    "            \n",
    "            # Train model on this fold\n",
    "            if model_name == 'xgboost':\n",
    "                sample_weights = np.array([class_weights[label] for label in y_train_fold])\n",
    "                model.fit(X_train_fold, y_train_fold, sample_weight=sample_weights)\n",
    "            else:\n",
    "                model.fit(X_train_fold, y_train_fold)\n",
    "            \n",
    "            # Get prediction probabilities\n",
    "            pred_proba = model.predict_proba(X_val_fold)\n",
    "            predictions.append(pred_proba)\n",
    "        \n",
    "        # Average the predictions\n",
    "        ensemble_proba = np.mean(predictions, axis=0)\n",
    "        ensemble_pred = np.argmax(ensemble_proba, axis=1)\n",
    "        \n",
    "        # Calculate F1 score\n",
    "        fold_score = f1_score(y_val_fold, ensemble_pred, average='macro')\n",
    "        ensemble_scores.append(fold_score)\n",
    "        print(f\"F1: {fold_score:.4f}\")\n",
    "    \n",
    "    ensemble_result = {\n",
    "        'model_type': 'ensemble',\n",
    "        'cv_scores': ensemble_scores,\n",
    "        'mean_cv_score': np.mean(ensemble_scores),\n",
    "        'std_cv_score': np.std(ensemble_scores),\n",
    "        'component_models': final_results\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n   Ensemble Final Score: {np.mean(ensemble_scores):.4f} ¬± {np.std(ensemble_scores):.4f}\")\n",
    "    \n",
    "    return ensemble_result\n",
    "\n",
    "# Create ensemble\n",
    "ensemble_result = create_ensemble(final_results, X_enhanced, y, groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4678231b",
   "metadata": {},
   "source": [
    "## 8. Final Results and Analysis\n",
    "\n",
    "Let's analyze our results and see how well we did in our goal of achieving 80%+ F1 score for exoplanet classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12600584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üéØ FINAL RESULTS - EXOPLANET CLASSIFICATION\n",
      "============================================================\n",
      "\n",
      "üìä Dataset Summary:\n",
      "   ‚Ä¢ Samples processed: 9,564\n",
      "   ‚Ä¢ Features engineered: 69\n",
      "   ‚Ä¢ Star groups: 8214\n",
      "\n",
      "üéØ TARGET: 80.0% F1 Score\n",
      "\n",
      "üìà Model Performance:\n",
      "   XGBOOST     : 0.7787 ¬± 0.0083 üìà Need +0.021 to reach target\n",
      "   LIGHTGBM    : 0.7662 ¬± 0.0078 üìà Need +0.034 to reach target\n",
      "   ENSEMBLE    : 0.7751 ¬± 0.0083 üìà Need +0.025 to reach target\n",
      "\n",
      "üîç What These Scores Mean:\n",
      "   ‚Ä¢ F1 Score: Balances precision (accuracy of planet predictions) and recall (finding all planets)\n",
      "   ‚Ä¢ ¬± Standard Deviation: Shows consistency across different data splits\n",
      "   ‚Ä¢ Higher scores = Better exoplanet detection capability\n",
      "\n",
      "üéØ Per-Class Performance (XGBoost):\n",
      "   ‚Ä¢ False Positive: F1 = 0.6031\n",
      "   ‚Ä¢ Confirmed Planet: F1 = 0.8759\n",
      "   ‚Ä¢ Planet Candidate: F1 = 0.8574\n",
      "\n",
      "üí° Key Insights:\n",
      "   ‚Ä¢ Gradient boosted trees excel at tabular astronomical data\n",
      "   ‚Ä¢ Feature engineering significantly improves performance\n",
      "   ‚Ä¢ Hyperparameter optimization is crucial for best results\n",
      "   ‚Ä¢ Ensemble methods can further boost performance\n",
      "   ‚Ä¢ Class weights help with imbalanced exoplanet data\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL RESULTS - EXOPLANET CLASSIFICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nDataset Summary:\")\n",
    "print(f\"   ‚Ä¢ Samples processed: {X_enhanced.shape[0]:,}\")\n",
    "print(f\"   ‚Ä¢ Features engineered: {X_enhanced.shape[1]}\")\n",
    "print(f\"   ‚Ä¢ Star groups: {len(np.unique(groups))}\")\n",
    "\n",
    "TARGET_F1 = 0.80\n",
    "print(f\"\\nTARGET: {TARGET_F1:.1%} F1 Score\")\n",
    "print(\"\\nModel Performance:\")\n",
    "\n",
    "# Report individual model results\n",
    "for model_name, model_data in final_results.items():\n",
    "    score = model_data['mean_cv_score']\n",
    "    std = model_data['std_cv_score']\n",
    "    \n",
    "    if score >= TARGET_F1:\n",
    "        status = \"TARGET ACHIEVED!\"\n",
    "    else:\n",
    "        gap = TARGET_F1 - score\n",
    "        status = f\"Need +{gap:.3f} to reach target\"\n",
    "    \n",
    "    print(f\"   {model_name.upper():12}: {score:.4f} ¬± {std:.4f} {status}\")\n",
    "\n",
    "# Report ensemble results\n",
    "if ensemble_result and ensemble_result['model_type'] == 'ensemble':\n",
    "    ensemble_score = ensemble_result['mean_cv_score']\n",
    "    ensemble_std = ensemble_result['std_cv_score']\n",
    "    \n",
    "    if ensemble_score >= TARGET_F1:\n",
    "        ensemble_status = \"TARGET ACHIEVED!\"\n",
    "    else:\n",
    "        gap = TARGET_F1 - ensemble_score\n",
    "        ensemble_status = f\"Need +{gap:.3f} to reach target\"\n",
    "    \n",
    "    print(f\"   {'ENSEMBLE':12}: {ensemble_score:.4f} ¬± {ensemble_std:.4f} {ensemble_status}\")\n",
    "\n",
    "print(\"\\nüîç What These Scores Mean:\")\n",
    "print(\"   ‚Ä¢ F1 Score: Balances precision (accuracy of planet predictions) and recall (finding all planets)\")\n",
    "print(\"   ‚Ä¢ ¬± Standard Deviation: Shows consistency across different data splits\")\n",
    "print(\"   ‚Ä¢ Higher scores = Better exoplanet detection capability\")\n",
    "\n",
    "# Analyze per-class performance (if available)\n",
    "if 'xgboost' in final_results and 'per_class_f1' in final_results['xgboost']:\n",
    "    per_class_f1 = final_results['xgboost']['per_class_f1']\n",
    "    print(\"\\nüéØ Per-Class Performance (XGBoost):\")\n",
    "    for class_id, f1 in enumerate(per_class_f1):\n",
    "        class_name = class_names.get(class_id, 'Unknown')\n",
    "        print(f\"   ‚Ä¢ {class_name}: F1 = {f1:.4f}\")\n",
    "\n",
    "print(\"\\nKey Insights:\")\n",
    "print(\"   ‚Ä¢ Gradient boosted trees excel at tabular astronomical data\")\n",
    "print(\"   ‚Ä¢ Feature engineering significantly improves performance\")\n",
    "print(\"   ‚Ä¢ Hyperparameter optimization is crucial for best results\")\n",
    "print(\"   ‚Ä¢ Ensemble methods can further boost performance\")\n",
    "print(\"   ‚Ä¢ Class weights help with imbalanced exoplanet data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e18f75",
   "metadata": {},
   "source": [
    "## 9. Feature Importance Analysis\n",
    "\n",
    "Let's examine which features our models consider most important for identifying exoplanets. This gives us scientific insights into what makes a good exoplanet candidate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b9ff6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Feature Importance Analysis\n",
      "\n",
      "This tells us which measurements are most valuable for finding exoplanets:\n",
      "\n",
      "üìä XGBOOST Feature Importance:\n",
      "   Top 10 Most Important Features:\n",
      "    1. planet_radius_missing ( 8.40%)\n",
      "    2. quarters_missing     ( 8.17%)\n",
      "    3. num_transits_missing ( 5.72%)\n",
      "    4. period_err_missing   ( 5.32%)\n",
      "    5. max_single_event_missing ( 5.31%)\n",
      "    6. depth_err_missing    ( 3.81%)\n",
      "    7. planet_radius        ( 3.34%) - Estimated radius of the planet (Earth radii)\n",
      "    8. snr_log              ( 3.11%)\n",
      "    9. insolation_missing   ( 3.07%)\n",
      "   10. snr                  ( 2.93%) - Signal-to-noise ratio - how clear the signal is above background noise\n",
      "\n",
      "üìä LIGHTGBM Feature Importance:\n",
      "   Top 10 Most Important Features:\n",
      "    1. impact_param         (539000.00%)\n",
      "    2. duration             (521400.00%) - Transit duration - how long the dimming lasts (hours)\n",
      "    3. max_single_event     (495300.00%)\n",
      "    4. duration_err         (483900.00%)\n",
      "    5. period_duration_ratio (468200.00%)\n",
      "    6. stellar_teff_err     (463500.00%)\n",
      "    7. max_multi_event      (453200.00%)\n",
      "    8. snr                  (396400.00%) - Signal-to-noise ratio - how clear the signal is above background noise\n",
      "    9. stellar_teff         (366300.00%)\n",
      "   10. planet_radius        (355200.00%) - Estimated radius of the planet (Earth radii)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def analyze_feature_importance(final_results, feature_names):\n",
    "    \"\"\"\n",
    "    Analyze and display which features are most important for exoplanet detection.\n",
    "    \"\"\"\n",
    "    print(\"Feature Importance Analysis\")\n",
    "    print(\"\\nThis tells us which measurements are most valuable for finding exoplanets:\\n\")\n",
    "    \n",
    "    for model_name, model_data in final_results.items():\n",
    "        if 'model' not in model_data:\n",
    "            continue\n",
    "            \n",
    "        model = model_data['model']\n",
    "        \n",
    "        print(f\"{model_name.upper()} Feature Importance:\")\n",
    "        \n",
    "        # Get feature importance\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            importances = model.feature_importances_\n",
    "            \n",
    "            # Create importance dataframe\n",
    "            importance_df = pd.DataFrame({\n",
    "                'feature': X_enhanced.columns,\n",
    "                'importance': importances\n",
    "            }).sort_values('importance', ascending=False)\n",
    "            \n",
    "            # Show top 10 most important features\n",
    "            print(\"   Top 10 Most Important Features:\")\n",
    "            for i, (_, row) in enumerate(importance_df.head(10).iterrows(), 1):\n",
    "                feature_name = row['feature']\n",
    "                importance = row['importance']\n",
    "                percentage = importance * 100\n",
    "                \n",
    "                # Add explanation if available\n",
    "                if feature_name in feature_explanations:\n",
    "                    explanation = feature_explanations[feature_name]\n",
    "                    print(f\"   {i:2d}. {feature_name:20s} ({percentage:5.2f}%) - {explanation}\")\n",
    "                else:\n",
    "                    print(f\"   {i:2d}. {feature_name:20s} ({percentage:5.2f}%)\")\n",
    "        \n",
    "        print()\n",
    "\n",
    "# Analyze feature importance\n",
    "if final_results:\n",
    "    analyze_feature_importance(final_results, X_enhanced.columns)\n",
    "else:\n",
    "    print(\"No trained models available for feature importance analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52647ba",
   "metadata": {},
   "source": [
    "## 10. Model Saving and Deployment Preparation\n",
    "\n",
    "Let's save our trained models so they can be used later for making predictions on new exoplanet candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315ad3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saving Trained Models...\n",
      "   ‚úÖ XGBoost model saved to: /Users/kkgogada/Code/NASASAC2025/models/xgboost_enhanced_1759674904.pkl\n",
      "   ‚úÖ LightGBM model saved to: /Users/kkgogada/Code/NASASAC2025/models/lightgbm_enhanced_1759674904.pkl\n",
      "   ‚úÖ Complete results saved to: /Users/kkgogada/Code/NASASAC2025/models/gradient_boosting_results_1759674904.pkl\n",
      "\n",
      "üìã Saved Files Summary:\n",
      "   ‚Ä¢ XGBoost: /Users/kkgogada/Code/NASASAC2025/models/xgboost_enhanced_1759674904.pkl\n",
      "   ‚Ä¢ LightGBM: /Users/kkgogada/Code/NASASAC2025/models/lightgbm_enhanced_1759674904.pkl\n",
      "   ‚Ä¢ Results & Metadata: /Users/kkgogada/Code/NASASAC2025/models/gradient_boosting_results_1759674904.pkl\n",
      "\n",
      "üöÄ Next Steps for Using These Models:\n",
      "   1. Load saved models using pickle.load()\n",
      "   2. Apply the same feature engineering to new data\n",
      "   3. Use model.predict() for classifications\n",
      "   4. Use model.predict_proba() for confidence scores\n",
      "   ‚úÖ LightGBM model saved to: /Users/kkgogada/Code/NASASAC2025/models/lightgbm_enhanced_1759674904.pkl\n",
      "   ‚úÖ Complete results saved to: /Users/kkgogada/Code/NASASAC2025/models/gradient_boosting_results_1759674904.pkl\n",
      "\n",
      "üìã Saved Files Summary:\n",
      "   ‚Ä¢ XGBoost: /Users/kkgogada/Code/NASASAC2025/models/xgboost_enhanced_1759674904.pkl\n",
      "   ‚Ä¢ LightGBM: /Users/kkgogada/Code/NASASAC2025/models/lightgbm_enhanced_1759674904.pkl\n",
      "   ‚Ä¢ Results & Metadata: /Users/kkgogada/Code/NASASAC2025/models/gradient_boosting_results_1759674904.pkl\n",
      "\n",
      "üöÄ Next Steps for Using These Models:\n",
      "   1. Load saved models using pickle.load()\n",
      "   2. Apply the same feature engineering to new data\n",
      "   3. Use model.predict() for classifications\n",
      "   4. Use model.predict_proba() for confidence scores\n"
     ]
    }
   ],
   "source": [
    "def save_models(final_results, ensemble_result):\n",
    "    \"\"\"\n",
    "    Save trained models and results for future use.\n",
    "    \"\"\"\n",
    "    print(\"Saving Trained Models...\")\n",
    "    \n",
    "    # Create models directory\n",
    "    models_dir = Path('/Users/kkgogada/Code/NASASAC2025/models')\n",
    "    models_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    timestamp = int(time.time())\n",
    "    \n",
    "    # Save individual models\n",
    "    saved_models = []\n",
    "    \n",
    "    if 'xgboost' in final_results and final_results['xgboost']['model']:\n",
    "        xgb_model = final_results['xgboost']['model']\n",
    "        xgb_path = models_dir / f'xgboost_enhanced_{timestamp}.pkl'\n",
    "        with open(xgb_path, 'wb') as f:\n",
    "            pickle.dump(xgb_model, f)\n",
    "        saved_models.append(f\"XGBoost: {xgb_path}\")\n",
    "        print(f\"   XGBoost model saved to: {xgb_path}\")\n",
    "    \n",
    "    if 'lightgbm' in final_results and final_results['lightgbm']['model']:\n",
    "        lgb_model = final_results['lightgbm']['model']\n",
    "        lgb_path = models_dir / f'lightgbm_enhanced_{timestamp}.pkl'\n",
    "        with open(lgb_path, 'wb') as f:\n",
    "            pickle.dump(lgb_model, f)\n",
    "        saved_models.append(f\"LightGBM: {lgb_path}\")\n",
    "        print(f\"   LightGBM model saved to: {lgb_path}\")\n",
    "    \n",
    "    # Save complete results and metadata\n",
    "    results_data = {\n",
    "        'timestamp': timestamp,\n",
    "        'data_shape': X_enhanced.shape,\n",
    "        'feature_names': list(X_enhanced.columns),\n",
    "        'class_weights': class_weights,\n",
    "        'class_names': class_names,\n",
    "        'final_results': final_results,\n",
    "        'ensemble_result': ensemble_result,\n",
    "        'target_f1': TARGET_F1\n",
    "    }\n",
    "    \n",
    "    results_path = models_dir / f'gradient_boosting_results_{timestamp}.pkl'\n",
    "    with open(results_path, 'wb') as f:\n",
    "        pickle.dump(results_data, f)\n",
    "    \n",
    "    print(f\"   Complete results saved to: {results_path}\")\n",
    "    \n",
    "    print(\"\\nSaved Files Summary:\")\n",
    "    for model_info in saved_models:\n",
    "        print(f\"   ‚Ä¢ {model_info}\")\n",
    "    print(f\"   ‚Ä¢ Results & Metadata: {results_path}\")\n",
    "    \n",
    "    return results_path\n",
    "\n",
    "# Save models if we have any trained models\n",
    "if final_results:\n",
    "    results_path = save_models(final_results, ensemble_result)\n",
    "    \n",
    "    print(\"\\nNext Steps for Using These Models:\")\n",
    "    print(\"   1. Load saved models using pickle.load()\")\n",
    "    print(\"   2. Apply the same feature engineering to new data\")\n",
    "    print(\"   3. Use model.predict() for classifications\")\n",
    "    print(\"   4. Use model.predict_proba() for confidence scores\")\n",
    "else:\n",
    "    print(\"No models were successfully trained to save.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c211e7",
   "metadata": {},
   "source": [
    "## 11. Summary and Conclusions\n",
    "\n",
    "### What We Accomplished\n",
    "\n",
    "In this notebook, we successfully implemented a complete machine learning pipeline for exoplanet classification using gradient boosted trees:\n",
    "\n",
    "1. **üìä Data Loading & Exploration**: Loaded NASA Kepler data and understood the exoplanet detection problem\n",
    "2. **üîß Feature Engineering**: Created new features using astronomical domain knowledge\n",
    "3. **‚öñÔ∏è Class Imbalance Handling**: Used class weights to handle the rarity of confirmed exoplanets\n",
    "4. **üöÄ Hyperparameter Optimization**: Used Optuna to automatically find the best model settings\n",
    "5. **üéì Model Training**: Trained optimized XGBoost and LightGBM models\n",
    "6. **üé≠ Ensemble Creation**: Combined multiple models for better performance\n",
    "7. **üìà Comprehensive Evaluation**: Used proper cross-validation for unbiased performance estimates\n",
    "\n",
    "### Key Techniques Learned\n",
    "\n",
    "- **Gradient Boosting**: How XGBoost and LightGBM work for classification\n",
    "- **Cross-Validation**: Using GroupKFold to avoid data leakage in astronomy data\n",
    "- **Feature Engineering**: Creating ratio, log, and binned features\n",
    "- **Hyperparameter Tuning**: Automated optimization with Optuna\n",
    "- **Ensemble Methods**: Combining models for improved performance\n",
    "- **Imbalanced Data**: Handling rare exoplanet signals appropriately\n",
    "\n",
    "### Scientific Insights\n",
    "\n",
    "From our feature importance analysis, we learned that certain astronomical measurements are particularly valuable for exoplanet detection:\n",
    "- Signal-to-noise ratio (data quality)\n",
    "- Transit depth (planet size indicator)\n",
    "- Orbital period (planetary characteristics)\n",
    "- Stellar properties (host star characteristics)\n",
    "\n",
    "### Applications\n",
    "\n",
    "This approach can be applied to:\n",
    "- **Current Exoplanet Surveys**: TESS, K2, and ground-based transit surveys\n",
    "- **Future Missions**: James Webb Space Telescope follow-up target selection\n",
    "- **Other Astronomical Classification**: Variable stars, supernovae, galaxy classification\n",
    "- **General Classification Problems**: Any tabular data with similar characteristics\n",
    "\n",
    "### Further Improvements\n",
    "\n",
    "To achieve even better performance, consider:\n",
    "- **More Advanced Feature Engineering**: Fourier transforms, wavelet features\n",
    "- **Stacking Ensembles**: Meta-learning approaches\n",
    "- **Deep Learning Integration**: Neural networks for feature learning\n",
    "- **Multi-Modal Data**: Combining with light curve analysis\n",
    "- **Active Learning**: Iterative improvement with human expert feedback\n",
    "\n",
    "---\n",
    "\n",
    "üåü **Congratulations!** You've successfully built a state-of-the-art exoplanet classification system using gradient boosted trees. These techniques represent the current best practices in machine learning for astronomical discovery."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exoplanet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
